{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  逻辑回归原理\n",
    "# 概念\n",
    "    原理公式推导：\n",
    "        https://blog.csdn.net/ligang_csdn/article/details/53838743\n",
    "logistic回归是一种广义线性模型（generalized    linear    model） ， 因此与多重线性回归分析有很多相同之处。 它们的模型形式基本上相同， 都具有 wx+b，其中w和b是待求参数， 其区别在于他们的因变量不同， 多重线性回归直接将wx+b作为因变量， 即y=wx+b， 而logistic回归则通过函数L将wx+b对应一个隐状态p， p=L(wx+b)，然后根据p与1-p的大小决定因变量的值。 如果L是logistic函数，就是logistic回归。\n",
    "\n",
    "# 原理\n",
    "Logistic Regression和Linear Regression的原理是相似的，按照我自己的理解，可以简单的描述为这样的过程：\n",
    "（1）模型的选择：选择一个预测函数\n",
    "找一个合适的预测函数（Andrew Ng的公开课中称为hypothesis），一般表示为h函数，该函数就是我们需要找的分类函数，它用来预测输入数据的判断结果。这个过程时非常关键的，需要对数据有一定的了解或分析，知道或者猜测预测函数的“大概”形式，比如是线性函数还是非线性函数。\n",
    "（2）构造一个损失函数（Cost函数），该函数表示预测的输出（h）与训练数据类别（y）之间的偏差，可以是二者之间的差（h-y）或者是其他的形式。综合考虑所有训练数据的“损失”，将Cost求和或者求平均，记为J(θ)函数，表示所有训练数据预测值与实际类别的偏差。\n",
    "（3）显然，J(θ)函数的值越小表示预测函数越准确（即h函数越准确），所以这一步需要做的是找到J(θ)函数的最小值。找函数的最小值有不同的方法，Logistic Regression实现时有的是梯度下降法（Gradient Descent）。\n",
    "# 公式\n",
    "- Sigmoid函数，也称为逻辑函数(Logistic function)：\n",
    "$$g(z) = \\frac{1}{1+e^{-z}}$$\n",
    "- 目标函数：\n",
    "$$h_{\\theta}(x) = g({\\theta}^Tx), g(z) = \\frac{1}{1+e^{-z}}$$\n",
    "即：\n",
    "$$h_{\\theta}(x) = \\frac{1}{ 1+e^{-\\theta^Tx} }$$\n",
    "其中，$x$为输入样本，$\\theta$是所求参数。\n",
    "- $$P(y=1|x;\\theta) = g({\\theta}^Tx) = \\frac{1}{1+e^{-\\theta^Tx}}$$\n",
    "给定$x$和$\\theta$的条件下$y=1$的概率。\n",
    "- 决策函数：\n",
    "$$y*=1, if P(y=1|x) > 0.5$$\n",
    "选择0.5作为阈值是一个一般的做法，实际应用时特定的情况可以选择不同阈值，如果对正例的判别准确性要求高，可以选择阈值大一些，对正例的召回要求高，则可以选择阈值小一些。\n",
    "- 损失函数：\n",
    "https://zhuanlan.zhihu.com/p/28415991\n",
    "\n",
    "- $$J(\\theta) = -\\frac{1}{m}\\sum_{i=1}^{m}[y^{(i)}\\log{h_{\\theta}(x^{(i)})}+ (1-y^{(i)})\\log{(1-h_{\\theta}(x^{(i)})}]$$\n",
    "- $$\\frac{\\partial J(\\theta)}{\\partial \\theta_j} = -\\frac{1}{m}\\sum_{i=1}^{m}[\\frac{y^{(i)}}{h_{\\theta}(x^{(i)})} - (1-y^{(i)})\\frac{1}{(1-h_{\\theta}(x^{(i)})}]\\frac{\\partial h_{\\theta}(x^{(i)})}{\\partial \\theta_j}$$\n",
    "- $\\Rightarrow$ $\\because$\n",
    "- 由 $g(z) = \\frac{1}{1+e^{-z}}$\n",
    "- $\\Rightarrow$\n",
    "- $$\\frac{\\partial g(z)}{\\partial z} = (\\frac{1}{1+e^{-z}})^\\prime = \\frac{e^{-z}}{(1+e^{-z})^2} = g(z)(1-g(z))$$\n",
    "- $$\\frac{\\partial h_{\\theta}(x^{(i)})}{\\partial \\theta_j} = \\frac{\\partial g(\\theta^Tx^{(i)})}{\\partial (\\theta^Tx^{(i)})}\\frac{\\partial (\\theta^Tx^{(i)})}{\\partial (\\theta_j)} = h_{\\theta}(x^{(i)})(1-h_{\\theta}(x^{(i)}))x^{(i)}_{j}$$\n",
    "\n",
    "- $$ \\frac{\\partial J(\\theta)}{\\partial \\theta_j} = -\\frac{1}{m}\\sum_{i=1}^{m}[y^{(i)}(1-h_{\\theta}(x^{(i)})) - (1-y^{(i)})h_{\\theta}(x^{(i)})]x^{(i)}_{j} $$\n",
    "$$= -\\frac{1}{m}\\sum_{i=1}^{m}(y^{(i)} - h_{\\theta}(x^{(i)}))x^{(i)}_{j}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
