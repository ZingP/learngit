{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 模型定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlexNet(nn.Module):\n",
    "    def __init__(self):#init函数定义的是网络的架构、关键的网络模块、模组\n",
    "        super(AlexNet,self).__init__()\n",
    "        self.feature_block=nn.Sequential(\n",
    "            nn.Conv2d(1,64,kernel_size=11,stride=4,padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3,stride=2),\n",
    "            nn.Conv2d(64,192,kernel_size=5,padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3,stride=2),\n",
    "            nn.Conv2d(192,384,kernel_size=3,padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(384,256,kernel_size=3,padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256,256,kernel_size=3,padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3,stride=2)\n",
    "        )\n",
    "        self.avgpool=nn.AdaptiveAvgPool2d((6,6))\n",
    "        self.class_block=nn.Sequential(\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(256*6*6,4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096,4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(4096,10),\n",
    "        )\n",
    "        \n",
    "    def forward(self,x):#数据的正向流\n",
    "        x = self.feature_block(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0),256*6*6)\n",
    "        x = self.class_block(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 模型训练与评估类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timer(func):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start = time.time()\n",
    "        func(*args,**kwargs)\n",
    "        end = time.time()\n",
    "        cost = end - start\n",
    "        print(\"Cost time: {} mins.\".format(cost/60)) \n",
    "    return wrapper\n",
    "\n",
    "class CNNModel(object):\n",
    "    def __init__(self, model, train_data, test_data, model_dir, model_name,\n",
    "                 best_valid_loss=float('inf'), n_split=0.9, batch_size=64, epochs=10):\n",
    "        self.batch_size = batch_size\n",
    "        self.epochs = epochs\n",
    "        self.best_valid_loss = best_valid_loss\n",
    "        self.model_dir = model_dir\n",
    "        self.model_name = model_name\n",
    "        self.n_split = n_split\n",
    "        \n",
    "        self.train_data =  train_data\n",
    "        self.test_data = test_data\n",
    "        \n",
    "        self.device = self.get_device()\n",
    "        self.init_data()\n",
    "        self.init_iterator()\n",
    "        self.init_model_path()\n",
    "        \n",
    "        self.model = self.init_model(model)\n",
    "        self.optimizer = self.set_optimizer()\n",
    "        self.criterion = self.set_criterion()\n",
    "        \n",
    "    def get_device(self):\n",
    "        d = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        return d\n",
    "    \n",
    "    def init_data(self):\n",
    "        n_train = int(len(self.train_data)*self.n_split)\n",
    "        n_validation = len(self.train_data) - n_train\n",
    "        self.train_data, self.valid_data = torch.utils.data.random_split(self.train_data, [n_train, n_validation])\n",
    "    \n",
    "    def init_iterator(self):\n",
    "        self.train_iterator = torch.utils.data.DataLoader(self.train_data, shuffle=True, batch_size=self.batch_size)\n",
    "        self.valid_iterator = torch.utils.data.DataLoader(self.valid_data, batch_size=self.batch_size)\n",
    "        self.test_iterator = torch.utils.data.DataLoader(self.test_data, batch_size=self.batch_size)\n",
    "        \n",
    "    def set_optimizer(self):\n",
    "        optimizer = optim.Adam(self.model.parameters()) \n",
    "        return optimizer\n",
    "    \n",
    "    def set_criterion(self):\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        return criterion\n",
    "    \n",
    "    def init_model(self, model):\n",
    "        if torch.cuda.device_count() > 1:\n",
    "            model = nn.DataParallel(model)\n",
    "        model = model.to(self.device)\n",
    "        return model\n",
    "        \n",
    "    def init_model_path(self):\n",
    "        if not os.path.isdir(self.model_dir):\n",
    "            os.makedirs(self.model_dir)\n",
    "        self.model_path = os.path.join(self.model_dir, self.model_name)\n",
    "        \n",
    "    # 定义评估函数\n",
    "    def accu(self, fx, y):\n",
    "        pred = fx.max(1,keepdim=True)[1]\n",
    "        correct = pred.eq(y.view_as(pred)).sum()  # 得到该batch的准确度\n",
    "        acc = correct.float()/pred.shape[0]\n",
    "        return acc\n",
    "\n",
    "    def train(self):\n",
    "        epoch_loss = 0   # 积累变量\n",
    "        epoch_acc = 0    # 积累变量\n",
    "        self.model.train()    # 该函数表示PHASE=Train\n",
    "\n",
    "        for (x,y) in self.train_iterator:  # 拿去每一个minibatch\n",
    "            x = x.to(self.device)\n",
    "            y = y.to(self.device)\n",
    "            self.optimizer.zero_grad()\n",
    "            fx = self.model(x)           # 进行forward\n",
    "            loss = self.criterion(fx,y)  # 计算Loss,train_loss\n",
    "            type(loss)\n",
    "            acc = self.accu(fx,y)      # 计算精确度，train_accu\n",
    "            loss.backward()     # 进行BP\n",
    "            self.optimizer.step()    # 统一更新模型\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "\n",
    "        return epoch_loss/len(self.train_iterator),epoch_acc/len(self.train_iterator)\n",
    "\n",
    "    def evaluate(self, iterator):\n",
    "        epoch_loss = 0\n",
    "        epoch_acc = 0\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            for (x,y) in iterator:\n",
    "                x = x.to(self.device)\n",
    "                y = y.to(self.device)\n",
    "                fx = self.model(x)\n",
    "                loss = self.criterion(fx,y)\n",
    "                acc = self.accu(fx,y)\n",
    "                epoch_loss += loss.item()\n",
    "                epoch_acc += acc.item()\n",
    "        return epoch_loss/len(iterator),epoch_acc/len(iterator)\n",
    "    \n",
    "    @timer\n",
    "    def train_fit(self):\n",
    "        info = 'Epoch:{0} | Train Loss:{1} | Train Acc:{2} | Val Loss:{3} | Val Acc:{4}'\n",
    "        for epoch in range(self.epochs):\n",
    "            train_loss, train_acc = self.train()\n",
    "            valid_loss, valid_acc = self.evaluate(self.valid_iterator)\n",
    "            if valid_loss < self.best_valid_loss:  # 如果是最好的模型就保存到文件夹\n",
    "                self.best_valid_loss = valid_loss\n",
    "                torch.save(self.model.state_dict(), self.model_path)\n",
    "            print(info.format(epoch+1, train_loss, train_acc, valid_loss, valid_acc))\n",
    "    \n",
    "    def get_acc(self):\n",
    "        self.model.load_state_dict(torch.load(self.model_path))\n",
    "        test_loss, test_acc = self.evaluate(self.test_iterator)\n",
    "        print('| Test Loss: {0} | Test Acc: {1} |'.format(test_loss,test_acc))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 数据集的准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_trans = transforms.Compose([\n",
    "    transforms.Resize(227),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = datasets.MNIST('data', train=True, download=True, transform=data_trans)\n",
    "test_data = datasets.MNIST('data', train=False, download=True, transform=data_trans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "n_split = 0.9\n",
    "batch_size = 64\n",
    "model_dir = 'models'\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "model_name = \"alexnet_mnist.pt\"\n",
    "model = AlexNet()\n",
    "\n",
    "obj = CNNModel(model=model, \n",
    "               train_data=train_data, \n",
    "               test_data=test_data, \n",
    "               model_dir=model_dir, \n",
    "               model_name=model_name,\n",
    "               best_valid_loss=best_valid_loss, \n",
    "               n_split=n_split, \n",
    "               batch_size=batch_size, \n",
    "               epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "print(obj.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataParallel(\n",
      "  (module): AlexNet(\n",
      "    (feature_block): Sequential(\n",
      "      (0): Conv2d(1, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
      "      (1): ReLU(inplace)\n",
      "      (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "      (4): ReLU(inplace)\n",
      "      (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (7): ReLU(inplace)\n",
      "      (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (9): ReLU(inplace)\n",
      "      (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (11): ReLU(inplace)\n",
      "      (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n",
      "    (class_block): Sequential(\n",
      "      (0): Dropout(p=0.5)\n",
      "      (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
      "      (2): ReLU(inplace)\n",
      "      (3): Dropout(p=0.5)\n",
      "      (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "      (5): ReLU(inplace)\n",
      "      (6): Linear(in_features=4096, out_features=10, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(obj.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:1 | Train Loss:0.2864568307046818 | Train Acc:0.906823904028436 | Val Loss:0.054361122085693034 | Val Acc:0.9864804966652647\n",
      "Epoch:2 | Train Loss:0.09301194253099544 | Train Acc:0.9737176639491348 | Val Loss:0.0781795164803043 | Val Acc:0.9780585106382979\n",
      "Epoch:3 | Train Loss:0.08140659090396352 | Train Acc:0.9769265896759892 | Val Loss:0.0447644844903249 | Val Acc:0.9872007978723404\n",
      "Epoch:4 | Train Loss:0.07166801897314558 | Train Acc:0.979771425797476 | Val Loss:0.03920154899675796 | Val Acc:0.9891400711333498\n",
      "Epoch:5 | Train Loss:0.05948051041813117 | Train Acc:0.9833135861214868 | Val Loss:0.037183631687088214 | Val Acc:0.9883643617021277\n",
      "Epoch:6 | Train Loss:0.055939722664972054 | Train Acc:0.9843379739336493 | Val Loss:0.04923080366991579 | Val Acc:0.9868683510638298\n",
      "Epoch:7 | Train Loss:0.05500357572521167 | Train Acc:0.9843935130331753 | Val Loss:0.04240083342735121 | Val Acc:0.9901374115588817\n",
      "Epoch:8 | Train Loss:0.050536542903561306 | Train Acc:0.9860350019982641 | Val Loss:0.03670251401180916 | Val Acc:0.9901374115588817\n",
      "Epoch:9 | Train Loss:0.04668690515152439 | Train Acc:0.9863744075829384 | Val Loss:0.04228511764519313 | Val Acc:0.9898603723404256\n",
      "Epoch:10 | Train Loss:0.04596478938668805 | Train Acc:0.987448163507109 | Val Loss:0.0374745466805836 | Val Acc:0.9911901595744681\n",
      "Cost time: 32.67839640776317 mins.\n"
     ]
    }
   ],
   "source": [
    "obj.train_fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Test Loss: 0.033057138240736 | Test Acc: 0.9903463375796179 |\n"
     ]
    }
   ],
   "source": [
    "obj.get_acc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
