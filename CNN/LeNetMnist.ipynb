{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 模型定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Pytorch中神经网络模块化接口nn的了解:\n",
    "\n",
    "torch.nn是专门为神经网络设计的模块化接口。nn构建于autograd之上，可以用来定义和运行神经网络。\n",
    "nn.Module是nn中十分重要的类,包含网络各层的定义及forward方法。\n",
    "定义自已的网络：\n",
    "    需要继承nn.Module类，并实现forward方法。\n",
    "    一般把网络中具有可学习参数的层放在构造函数__init__()中，\n",
    "    不具有可学习参数的层(如ReLU)可放在构造函数中，也可不放在构造函数中(而在forward中使用nn.functional来代替)\n",
    "    \n",
    "    只要在nn.Module的子类中定义了forward函数，backward函数就会被自动实现(利用Autograd)。\n",
    "    在forward函数中可以使用任何Variable支持的函数，毕竟在整个pytorch构建的图中，是Variable在流动。还可以使用\n",
    "    if,for,print,log等python语法.\n",
    "    \n",
    "    注：Pytorch基于nn.Module构建的模型中，只支持mini-batch的Variable输入方式，\n",
    "    比如，只有一张输入图片，也需要变成 N x C x H x W 的形式：\n",
    "    \n",
    "    input_image = torch.FloatTensor(1, 28, 28)\n",
    "    input_image = Variable(input_image)\n",
    "    input_image = input_image.unsqueeze(0)   # 1 x 1 x 28 x 28\n",
    "    \n",
    "    二维卷积层, 输入的尺度是(N, C_in,H,W)，输出尺度（N,C_out,H_out,W_out）的计算方式\n",
    "    torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True)\n",
    "    in_channels(int) – 输入信号的通道\n",
    "    out_channels(int) – 卷积产生的通道\n",
    "    kerner_size(int or tuple) - 卷积核的尺寸\n",
    "    stride(int or tuple, optional) - 卷积步长\n",
    "    padding(int or tuple, optional) - 输入的每一条边补充0的层数\n",
    "    dilation(int or tuple, optional) – 卷积核元素之间的间距\n",
    "    groups(int, optional) – 从输入通道到输出通道的阻塞连接数\n",
    "    bias(bool, optional) - 如果bias=True，添加偏置\n",
    "    \n",
    "    \n",
    "\"\"\"\n",
    "class LeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        # nn.Module的子类函数必须在构造函数中执行父类的构造函数\n",
    "        super(LeNet, self).__init__()\n",
    "        # nn.Conv2d返回的是一个Conv2d class的一个对象，该类中包含forward函数的实现\n",
    "        # 当调用self.conv1(input)的时候，就会调用该类的forward函数\n",
    "        # 第一层conv1卷积层，in_channel=1,output_channel=6,kernel_size=5*5,input_size=32*32,output_size=28*28\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        # 第二层conv2，output_channel=6, kernel 5*5, output_size=10*10,input_size=14*14\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16*5*5, 120)\n",
    "        self.fc2 = nn.Linear(120, 80)\n",
    "        self.fc3 = nn.Linear(80, 10) # 不用增加softmax层，在cross_entropy的Loss中自动增加了Softmax\n",
    "       \n",
    "    # 定义了每次执行的 计算步骤。在所有的子类中都需要重写这个函数\n",
    "    def forward(self,x):\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), 2)   # F.max_pool2d的返回值是一个Variable\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = x.view(x.shape[0], -1)  # 返回一个有相同数据但大小不同的tensor。 返回的tensor必须有与原tensor相同的数据和相同数目的元素，但可以有不同的大小。一个tensor必须是连续的contiguous()才能被查看\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 模型训练与评估类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timer(func):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start = time.time()\n",
    "        func(*args,**kwargs)\n",
    "        end = time.time()\n",
    "        cost = end - start\n",
    "        print(\"Cost time: {} mins.\".format(cost/60)) \n",
    "    return wrapper\n",
    "\n",
    "class CNNModel(object):\n",
    "    def __init__(self, model, train_data, test_data, model_dir, model_name,\n",
    "                 best_valid_loss=float('inf'), n_split=0.9, batch_size=64, epochs=10):\n",
    "        self.batch_size = batch_size\n",
    "        self.epochs = epochs\n",
    "        self.best_valid_loss = best_valid_loss\n",
    "        self.model_dir = model_dir\n",
    "        self.model_name = model_name\n",
    "        self.n_split = n_split\n",
    "        \n",
    "        self.train_data =  train_data\n",
    "        self.test_data = test_data\n",
    "        \n",
    "        self.device = self.get_device()\n",
    "        self.init_data()\n",
    "        self.init_iterator()\n",
    "        self.init_model_path()\n",
    "        \n",
    "        self.model = self.init_model(model)\n",
    "        self.optimizer = self.set_optimizer()\n",
    "        self.criterion = self.set_criterion()\n",
    "        \n",
    "    def get_device(self):\n",
    "        d = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        return d\n",
    "    \n",
    "    def init_data(self):\n",
    "        n_train = int(len(self.train_data)*self.n_split)\n",
    "        n_validation = len(self.train_data) - n_train\n",
    "        self.train_data, self.valid_data = torch.utils.data.random_split(self.train_data, [n_train, n_validation])\n",
    "    \n",
    "    def init_iterator(self):\n",
    "        self.train_iterator = torch.utils.data.DataLoader(self.train_data, shuffle=True, batch_size=self.batch_size)\n",
    "        self.valid_iterator = torch.utils.data.DataLoader(self.valid_data, batch_size=self.batch_size)\n",
    "        self.test_iterator = torch.utils.data.DataLoader(self.test_data, batch_size=self.batch_size)\n",
    "        \n",
    "    def set_optimizer(self):\n",
    "        optimizer = optim.Adam(self.model.parameters()) \n",
    "        return optimizer\n",
    "    \n",
    "    def set_criterion(self):\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        return criterion\n",
    "    \n",
    "    def init_model(self, model):\n",
    "        if torch.cuda.device_count() > 1:\n",
    "            model = nn.DataParallel(model)\n",
    "        model = model.to(self.device)\n",
    "        return model\n",
    "        \n",
    "    def init_model_path(self):\n",
    "        if not os.path.isdir(self.model_dir):\n",
    "            os.makedirs(self.model_dir)\n",
    "        self.model_path = os.path.join(self.model_dir, self.model_name)\n",
    "        \n",
    "    # 定义评估函数\n",
    "    def accu(self, fx, y):\n",
    "        pred = fx.max(1, keepdim=True)[1]\n",
    "        correct = pred.eq(y.view_as(pred)).sum()  # 得到该batch的准确度\n",
    "        acc = correct.float()/pred.shape[0]\n",
    "        return acc\n",
    "\n",
    "    def train(self):\n",
    "        epoch_loss = 0   # 积累变量\n",
    "        epoch_acc = 0    # 积累变量\n",
    "        self.model.train()    # 该函数表示PHASE=Train\n",
    "\n",
    "        for (x,y) in self.train_iterator:  # 拿去每一个minibatch\n",
    "            x = x.to(self.device)\n",
    "            y = y.to(self.device)\n",
    "            self.optimizer.zero_grad()\n",
    "            fx = self.model(x)           # 进行forward\n",
    "            loss = self.criterion(fx,y)  # 计算Loss,train_loss\n",
    "            type(loss)\n",
    "            acc = self.accu(fx,y)    # 计算精确度，train_accu\n",
    "            loss.backward()          # 进行BP\n",
    "            self.optimizer.step()    # 统一更新模型\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "\n",
    "        return epoch_loss/len(self.train_iterator),epoch_acc/len(self.train_iterator)\n",
    "\n",
    "    def evaluate(self, iterator):\n",
    "        epoch_loss = 0\n",
    "        epoch_acc = 0\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            for (x,y) in iterator:\n",
    "                x = x.to(self.device)\n",
    "                y = y.to(self.device)\n",
    "                fx = self.model(x)\n",
    "                loss = self.criterion(fx,y)\n",
    "                acc = self.accu(fx,y)\n",
    "                epoch_loss += loss.item()\n",
    "                epoch_acc += acc.item()\n",
    "        return epoch_loss/len(iterator),epoch_acc/len(iterator)\n",
    "    \n",
    "    @timer\n",
    "    def train_fit(self):\n",
    "        info = 'Epoch:{0} | Train Loss:{1} | Train Acc:{2} | Val Loss:{3} | Val Acc:{4}'\n",
    "        for epoch in range(self.epochs):\n",
    "            train_loss, train_acc = self.train()\n",
    "            valid_loss, valid_acc = self.evaluate(self.valid_iterator)\n",
    "            if valid_loss < self.best_valid_loss:  # 如果是最好的模型就保存到文件夹\n",
    "                self.best_valid_loss = valid_loss\n",
    "                torch.save(self.model.state_dict(), self.model_path)\n",
    "            print(info.format(epoch+1, train_loss, train_acc, valid_loss, valid_acc))\n",
    "    \n",
    "    def get_acc(self):\n",
    "        self.model.load_state_dict(torch.load(self.model_path))\n",
    "        test_loss, test_acc = self.evaluate(self.test_iterator)\n",
    "        print('| Test Loss: {0} | Test Acc: {1} |'.format(test_loss,test_acc))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 数据集的准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_trans = transforms.Compose([\n",
    "    transforms.Resize(32),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = datasets.MNIST('data', train=True, download=True, transform=data_trans)\n",
    "test_data = datasets.MNIST('data', train=False, download=True, transform=data_trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1\n",
    "n_split = 0.9\n",
    "batch_size = 64\n",
    "model_dir = 'models'\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "model_name = \"lenet_mnist.pt\"\n",
    "model = LeNet()\n",
    "\n",
    "obj = CNNModel(model=model, \n",
    "               train_data=train_data, \n",
    "               test_data=test_data, \n",
    "               model_dir=model_dir, \n",
    "               model_name=model_name,\n",
    "               best_valid_loss=best_valid_loss, \n",
    "               n_split=n_split, \n",
    "               batch_size=batch_size, \n",
    "               epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name: conv1.weight, param: Parameter containing:\n",
      "tensor([[[[ 0.0589, -0.0283,  0.1930,  0.1327, -0.0820],\n",
      "          [-0.0741, -0.0690,  0.0288, -0.0728,  0.0379],\n",
      "          [ 0.0830,  0.1200,  0.0167,  0.0064,  0.0377],\n",
      "          [ 0.0174, -0.0979,  0.0465,  0.1862,  0.1280],\n",
      "          [ 0.1560, -0.0336, -0.0894, -0.1760, -0.1037]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1907,  0.0524, -0.1414, -0.0559, -0.1851],\n",
      "          [-0.1228,  0.1052,  0.0046,  0.0253, -0.0967],\n",
      "          [-0.0429,  0.1239,  0.1373, -0.1639, -0.0494],\n",
      "          [ 0.0752, -0.0897, -0.1574, -0.1168,  0.1470],\n",
      "          [-0.0850, -0.1789, -0.0055,  0.0314, -0.1272]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1110, -0.1656,  0.1073,  0.0823, -0.0452],\n",
      "          [ 0.1121, -0.1560, -0.0929, -0.1319, -0.0042],\n",
      "          [ 0.0045,  0.0822,  0.1764,  0.1985, -0.1017],\n",
      "          [-0.0646,  0.1559,  0.0980, -0.1105,  0.0501],\n",
      "          [ 0.0624,  0.0212,  0.1089,  0.0460, -0.1877]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1006,  0.0958,  0.0620,  0.0621, -0.1434],\n",
      "          [-0.1676,  0.0610,  0.0537,  0.0355,  0.1520],\n",
      "          [-0.0622, -0.1512,  0.0330,  0.1949, -0.1788],\n",
      "          [-0.1599,  0.1162,  0.1211, -0.1949,  0.0104],\n",
      "          [ 0.0068, -0.1612,  0.1642,  0.1228,  0.0681]]],\n",
      "\n",
      "\n",
      "        [[[-0.1533, -0.0634,  0.0052, -0.1661, -0.0423],\n",
      "          [-0.1580, -0.0114, -0.1890,  0.1685, -0.1807],\n",
      "          [ 0.1258, -0.0367, -0.1894, -0.0350,  0.1922],\n",
      "          [ 0.1084, -0.0205, -0.1397, -0.0850, -0.0963],\n",
      "          [-0.1563, -0.1275, -0.1670,  0.0419,  0.1475]]],\n",
      "\n",
      "\n",
      "        [[[-0.1780, -0.0179,  0.1290, -0.0112, -0.1105],\n",
      "          [-0.1860, -0.1463, -0.1868, -0.0659, -0.1614],\n",
      "          [ 0.0779,  0.0430, -0.1311, -0.1721, -0.1821],\n",
      "          [ 0.1732, -0.1697,  0.0211, -0.0118, -0.1442],\n",
      "          [ 0.1610,  0.0231, -0.1780, -0.0908, -0.1647]]]], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "name: conv1.bias, param: Parameter containing:\n",
      "tensor([-0.0071,  0.0131,  0.1545,  0.0766, -0.1355, -0.0213], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "name: conv2.weight, param: Parameter containing:\n",
      "tensor([[[[ 0.0542, -0.0086, -0.0183, -0.0579,  0.0426],\n",
      "          [-0.0389, -0.0359,  0.0015,  0.0094,  0.0653],\n",
      "          [ 0.0082, -0.0464,  0.0715, -0.0194, -0.0286],\n",
      "          [-0.0492,  0.0606, -0.0521,  0.0243,  0.0006],\n",
      "          [ 0.0655, -0.0413, -0.0424,  0.0600, -0.0181]],\n",
      "\n",
      "         [[-0.0351, -0.0165,  0.0289, -0.0269,  0.0053],\n",
      "          [-0.0731,  0.0255,  0.0358,  0.0655, -0.0783],\n",
      "          [-0.0124, -0.0708, -0.0745,  0.0447, -0.0633],\n",
      "          [-0.0330, -0.0348,  0.0391, -0.0573,  0.0738],\n",
      "          [-0.0150, -0.0145,  0.0792,  0.0652,  0.0763]],\n",
      "\n",
      "         [[ 0.0510,  0.0693, -0.0493,  0.0623,  0.0788],\n",
      "          [-0.0401, -0.0168, -0.0002, -0.0127, -0.0759],\n",
      "          [-0.0807, -0.0456, -0.0510, -0.0093,  0.0261],\n",
      "          [-0.0600,  0.0181,  0.0456, -0.0638,  0.0115],\n",
      "          [-0.0097, -0.0464,  0.0004, -0.0191, -0.0003]],\n",
      "\n",
      "         [[ 0.0426, -0.0611,  0.0385, -0.0064,  0.0278],\n",
      "          [ 0.0030,  0.0761,  0.0562,  0.0215, -0.0744],\n",
      "          [-0.0665, -0.0598, -0.0732, -0.0364,  0.0005],\n",
      "          [-0.0394, -0.0259, -0.0188,  0.0440,  0.0649],\n",
      "          [-0.0328, -0.0059,  0.0027, -0.0693, -0.0402]],\n",
      "\n",
      "         [[-0.0136,  0.0054,  0.0235,  0.0081, -0.0573],\n",
      "          [ 0.0551,  0.0455,  0.0559, -0.0729,  0.0117],\n",
      "          [ 0.0138, -0.0532,  0.0382,  0.0419, -0.0678],\n",
      "          [-0.0204,  0.0152,  0.0349, -0.0032, -0.0347],\n",
      "          [-0.0410, -0.0815, -0.0748,  0.0805,  0.0419]],\n",
      "\n",
      "         [[ 0.0229,  0.0430,  0.0436, -0.0230,  0.0746],\n",
      "          [ 0.0479,  0.0403,  0.0189, -0.0408,  0.0130],\n",
      "          [-0.0204, -0.0391, -0.0329, -0.0576,  0.0403],\n",
      "          [ 0.0696,  0.0135,  0.0404,  0.0723, -0.0749],\n",
      "          [-0.0132, -0.0716,  0.0370, -0.0570, -0.0505]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0215,  0.0339, -0.0392, -0.0536,  0.0223],\n",
      "          [ 0.0385, -0.0022,  0.0536, -0.0238, -0.0792],\n",
      "          [ 0.0428,  0.0417,  0.0248,  0.0555, -0.0277],\n",
      "          [-0.0371,  0.0694,  0.0239, -0.0475, -0.0324],\n",
      "          [ 0.0558,  0.0262,  0.0758,  0.0773, -0.0498]],\n",
      "\n",
      "         [[ 0.0321, -0.0578, -0.0576,  0.0655,  0.0492],\n",
      "          [ 0.0702, -0.0295,  0.0232,  0.0120, -0.0537],\n",
      "          [ 0.0166, -0.0745, -0.0007, -0.0760, -0.0553],\n",
      "          [ 0.0202,  0.0033,  0.0623,  0.0318, -0.0021],\n",
      "          [ 0.0507,  0.0316, -0.0685, -0.0422,  0.0441]],\n",
      "\n",
      "         [[ 0.0088,  0.0590, -0.0805, -0.0769,  0.0132],\n",
      "          [ 0.0441, -0.0313,  0.0753,  0.0643, -0.0196],\n",
      "          [ 0.0127, -0.0509, -0.0700, -0.0117,  0.0015],\n",
      "          [ 0.0576,  0.0200,  0.0543,  0.0063, -0.0205],\n",
      "          [ 0.0660, -0.0308,  0.0524,  0.0546,  0.0055]],\n",
      "\n",
      "         [[ 0.0282,  0.0737,  0.0401,  0.0182, -0.0282],\n",
      "          [-0.0801,  0.0205, -0.0114, -0.0062,  0.0037],\n",
      "          [ 0.0479, -0.0375, -0.0766, -0.0647,  0.0605],\n",
      "          [-0.0268, -0.0073, -0.0756,  0.0701, -0.0180],\n",
      "          [-0.0075, -0.0775,  0.0043,  0.0194, -0.0510]],\n",
      "\n",
      "         [[-0.0794, -0.0189, -0.0184, -0.0702,  0.0771],\n",
      "          [ 0.0250,  0.0042,  0.0436, -0.0212,  0.0351],\n",
      "          [ 0.0599,  0.0685,  0.0793,  0.0214, -0.0315],\n",
      "          [-0.0496, -0.0549, -0.0307,  0.0574,  0.0665],\n",
      "          [-0.0711, -0.0275,  0.0719,  0.0086,  0.0208]],\n",
      "\n",
      "         [[-0.0349, -0.0322, -0.0758, -0.0571,  0.0448],\n",
      "          [-0.0452, -0.0122,  0.0045,  0.0371,  0.0316],\n",
      "          [-0.0445,  0.0225,  0.0208,  0.0682,  0.0192],\n",
      "          [ 0.0124,  0.0181, -0.0535,  0.0112,  0.0289],\n",
      "          [ 0.0135,  0.0704, -0.0593, -0.0219, -0.0194]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0103,  0.0422, -0.0095, -0.0055, -0.0605],\n",
      "          [ 0.0369,  0.0568, -0.0776,  0.0595, -0.0237],\n",
      "          [ 0.0249,  0.0142,  0.0331, -0.0140, -0.0743],\n",
      "          [ 0.0799,  0.0587, -0.0112, -0.0305, -0.0160],\n",
      "          [ 0.0344,  0.0157,  0.0093,  0.0356, -0.0022]],\n",
      "\n",
      "         [[-0.0803, -0.0672,  0.0355, -0.0101, -0.0754],\n",
      "          [ 0.0728, -0.0268,  0.0686,  0.0663,  0.0508],\n",
      "          [-0.0544, -0.0243,  0.0267, -0.0792,  0.0015],\n",
      "          [ 0.0746, -0.0177, -0.0451, -0.0385,  0.0609],\n",
      "          [ 0.0733,  0.0466,  0.0581, -0.0561, -0.0053]],\n",
      "\n",
      "         [[ 0.0276, -0.0744, -0.0516, -0.0418, -0.0312],\n",
      "          [-0.0785,  0.0072,  0.0518, -0.0115,  0.0721],\n",
      "          [ 0.0469, -0.0625, -0.0370,  0.0751,  0.0706],\n",
      "          [ 0.0350,  0.0666, -0.0804, -0.0377, -0.0048],\n",
      "          [ 0.0712, -0.0654,  0.0494, -0.0670, -0.0725]],\n",
      "\n",
      "         [[ 0.0257,  0.0084,  0.0533, -0.0255, -0.0718],\n",
      "          [-0.0545,  0.0114, -0.0553,  0.0018,  0.0136],\n",
      "          [-0.0699, -0.0438,  0.0697,  0.0131, -0.0542],\n",
      "          [-0.0745, -0.0574, -0.0087,  0.0489, -0.0506],\n",
      "          [-0.0278,  0.0306,  0.0513, -0.0098,  0.0561]],\n",
      "\n",
      "         [[-0.0142,  0.0225, -0.0177,  0.0445,  0.0678],\n",
      "          [-0.0813,  0.0644, -0.0679, -0.0148, -0.0328],\n",
      "          [-0.0359,  0.0311, -0.0404, -0.0033,  0.0666],\n",
      "          [ 0.0422,  0.0292, -0.0054,  0.0392,  0.0551],\n",
      "          [ 0.0734,  0.0435,  0.0345, -0.0280,  0.0057]],\n",
      "\n",
      "         [[ 0.0757, -0.0557, -0.0562, -0.0332, -0.0689],\n",
      "          [-0.0139, -0.0147,  0.0508,  0.0503,  0.0450],\n",
      "          [ 0.0405, -0.0091, -0.0371,  0.0267,  0.0188],\n",
      "          [ 0.0809,  0.0187,  0.0732,  0.0291,  0.0241],\n",
      "          [ 0.0142, -0.0568,  0.0054,  0.0649, -0.0448]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 0.0788,  0.0155, -0.0251,  0.0774, -0.0404],\n",
      "          [ 0.0688, -0.0632,  0.0339, -0.0750, -0.0795],\n",
      "          [ 0.0118,  0.0198,  0.0308,  0.0437, -0.0234],\n",
      "          [ 0.0505,  0.0018,  0.0605,  0.0614,  0.0049],\n",
      "          [ 0.0367,  0.0385, -0.0444,  0.0501, -0.0565]],\n",
      "\n",
      "         [[-0.0812, -0.0514,  0.0707, -0.0812,  0.0130],\n",
      "          [-0.0399,  0.0130, -0.0420, -0.0108,  0.0234],\n",
      "          [-0.0270, -0.0799, -0.0404,  0.0197, -0.0622],\n",
      "          [ 0.0513,  0.0202,  0.0434,  0.0715,  0.0450],\n",
      "          [-0.0320,  0.0607,  0.0181, -0.0288, -0.0788]],\n",
      "\n",
      "         [[ 0.0484, -0.0427, -0.0629, -0.0124, -0.0413],\n",
      "          [-0.0497, -0.0396,  0.0518, -0.0374, -0.0267],\n",
      "          [ 0.0625,  0.0738, -0.0580,  0.0777,  0.0022],\n",
      "          [ 0.0731, -0.0731,  0.0617, -0.0263,  0.0250],\n",
      "          [ 0.0335,  0.0247, -0.0373,  0.0363,  0.0655]],\n",
      "\n",
      "         [[ 0.0813,  0.0795, -0.0618,  0.0665, -0.0413],\n",
      "          [ 0.0263,  0.0438,  0.0805,  0.0554, -0.0301],\n",
      "          [ 0.0116, -0.0772,  0.0162, -0.0391,  0.0277],\n",
      "          [ 0.0673,  0.0154,  0.0191,  0.0345, -0.0567],\n",
      "          [ 0.0103, -0.0097, -0.0408,  0.0563,  0.0816]],\n",
      "\n",
      "         [[ 0.0617,  0.0806,  0.0653, -0.0025, -0.0363],\n",
      "          [-0.0003, -0.0091,  0.0119,  0.0142,  0.0423],\n",
      "          [-0.0470,  0.0780,  0.0007,  0.0220,  0.0530],\n",
      "          [-0.0197, -0.0451,  0.0271,  0.0137,  0.0221],\n",
      "          [-0.0049,  0.0181, -0.0152,  0.0152, -0.0640]],\n",
      "\n",
      "         [[ 0.0025,  0.0328, -0.0444,  0.0139,  0.0350],\n",
      "          [-0.0314, -0.0153, -0.0563,  0.0449, -0.0059],\n",
      "          [-0.0480,  0.0382, -0.0525, -0.0204,  0.0482],\n",
      "          [ 0.0467, -0.0543, -0.0079,  0.0398, -0.0501],\n",
      "          [ 0.0113, -0.0483,  0.0322,  0.0518,  0.0222]]],\n",
      "\n",
      "\n",
      "        [[[-0.0294, -0.0330, -0.0515,  0.0242, -0.0220],\n",
      "          [-0.0310,  0.0182, -0.0339, -0.0503,  0.0349],\n",
      "          [-0.0006,  0.0397, -0.0758, -0.0238,  0.0655],\n",
      "          [-0.0771, -0.0064, -0.0361,  0.0680, -0.0207],\n",
      "          [-0.0748,  0.0653,  0.0482,  0.0417,  0.0749]],\n",
      "\n",
      "         [[-0.0371, -0.0118,  0.0536,  0.0080, -0.0447],\n",
      "          [-0.0673,  0.0391,  0.0115,  0.0606, -0.0058],\n",
      "          [-0.0762, -0.0053,  0.0013,  0.0034, -0.0773],\n",
      "          [-0.0482, -0.0105,  0.0589, -0.0386,  0.0226],\n",
      "          [-0.0116,  0.0002, -0.0272, -0.0092, -0.0287]],\n",
      "\n",
      "         [[ 0.0172,  0.0305,  0.0142, -0.0169,  0.0425],\n",
      "          [ 0.0213, -0.0137,  0.0717,  0.0608, -0.0658],\n",
      "          [ 0.0566,  0.0267,  0.0655,  0.0114,  0.0791],\n",
      "          [-0.0672, -0.0814, -0.0771, -0.0138, -0.0806],\n",
      "          [ 0.0775,  0.0800,  0.0655,  0.0167, -0.0105]],\n",
      "\n",
      "         [[ 0.0521,  0.0801,  0.0531, -0.0763, -0.0334],\n",
      "          [ 0.0764,  0.0114,  0.0394,  0.0011, -0.0327],\n",
      "          [-0.0026, -0.0703, -0.0320,  0.0528,  0.0495],\n",
      "          [ 0.0276, -0.0755,  0.0501, -0.0481, -0.0532],\n",
      "          [-0.0128,  0.0555, -0.0377, -0.0194, -0.0226]],\n",
      "\n",
      "         [[ 0.0049, -0.0124,  0.0679, -0.0254,  0.0224],\n",
      "          [-0.0132, -0.0436,  0.0404, -0.0128,  0.0430],\n",
      "          [ 0.0552, -0.0157, -0.0121,  0.0315, -0.0637],\n",
      "          [ 0.0796,  0.0233,  0.0082, -0.0675,  0.0194],\n",
      "          [-0.0068,  0.0153, -0.0711,  0.0011, -0.0458]],\n",
      "\n",
      "         [[-0.0418,  0.0701,  0.0148,  0.0071,  0.0417],\n",
      "          [ 0.0254,  0.0442, -0.0416, -0.0665, -0.0468],\n",
      "          [ 0.0114, -0.0004,  0.0709,  0.0569, -0.0194],\n",
      "          [-0.0006, -0.0668,  0.0199,  0.0450, -0.0032],\n",
      "          [ 0.0095, -0.0404, -0.0545,  0.0386,  0.0452]]],\n",
      "\n",
      "\n",
      "        [[[-0.0123,  0.0428,  0.0507, -0.0325,  0.0379],\n",
      "          [ 0.0337,  0.0803, -0.0573,  0.0811,  0.0031],\n",
      "          [-0.0769, -0.0792, -0.0813, -0.0042,  0.0431],\n",
      "          [-0.0787, -0.0213,  0.0680, -0.0131,  0.0437],\n",
      "          [-0.0679,  0.0652, -0.0728, -0.0651,  0.0792]],\n",
      "\n",
      "         [[ 0.0814, -0.0349, -0.0615, -0.0341, -0.0315],\n",
      "          [-0.0433,  0.0334,  0.0733, -0.0027, -0.0660],\n",
      "          [ 0.0046,  0.0211, -0.0121,  0.0431, -0.0169],\n",
      "          [-0.0488, -0.0624, -0.0272, -0.0231, -0.0068],\n",
      "          [ 0.0205,  0.0399,  0.0251, -0.0650, -0.0153]],\n",
      "\n",
      "         [[-0.0425, -0.0433,  0.0086,  0.0270,  0.0313],\n",
      "          [ 0.0246, -0.0029, -0.0509, -0.0626, -0.0559],\n",
      "          [-0.0421, -0.0721, -0.0626,  0.0256, -0.0123],\n",
      "          [ 0.0666,  0.0072,  0.0331, -0.0454,  0.0146],\n",
      "          [ 0.0418,  0.0129,  0.0236,  0.0253,  0.0653]],\n",
      "\n",
      "         [[ 0.0244, -0.0764, -0.0146,  0.0188, -0.0696],\n",
      "          [ 0.0202,  0.0488, -0.0812,  0.0679,  0.0414],\n",
      "          [-0.0436,  0.0631, -0.0568, -0.0463,  0.0237],\n",
      "          [ 0.0280,  0.0477, -0.0260, -0.0278, -0.0282],\n",
      "          [-0.0085,  0.0537,  0.0466,  0.0553, -0.0089]],\n",
      "\n",
      "         [[ 0.0328,  0.0368, -0.0099, -0.0351,  0.0133],\n",
      "          [ 0.0433,  0.0123, -0.0399, -0.0024, -0.0752],\n",
      "          [ 0.0496,  0.0752, -0.0262, -0.0069, -0.0311],\n",
      "          [ 0.0647, -0.0264,  0.0611,  0.0447, -0.0307],\n",
      "          [ 0.0056, -0.0315, -0.0535, -0.0517,  0.0653]],\n",
      "\n",
      "         [[-0.0637,  0.0136, -0.0227,  0.0652,  0.0329],\n",
      "          [ 0.0018, -0.0637, -0.0391, -0.0478, -0.0037],\n",
      "          [ 0.0326,  0.0681,  0.0360,  0.0680,  0.0140],\n",
      "          [ 0.0017,  0.0751, -0.0785, -0.0104, -0.0591],\n",
      "          [-0.0386,  0.0552,  0.0462,  0.0405, -0.0289]]]], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "name: conv2.bias, param: Parameter containing:\n",
      "tensor([ 0.0538,  0.0051, -0.0674, -0.0760,  0.0357, -0.0309,  0.0690,  0.0206,\n",
      "        -0.0038, -0.0017,  0.0616,  0.0646,  0.0618,  0.0669, -0.0653,  0.0403],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "name: fc1.weight, param: Parameter containing:\n",
      "tensor([[-1.1494e-02,  4.8089e-02, -2.3162e-02,  ...,  1.4707e-04,\n",
      "          2.7237e-02,  1.1509e-02],\n",
      "        [-4.2897e-02, -4.3529e-02,  1.1844e-02,  ..., -2.6180e-02,\n",
      "         -4.6100e-02,  8.9881e-03],\n",
      "        [ 3.0036e-02, -4.2197e-02, -1.8834e-02,  ...,  2.0368e-02,\n",
      "         -1.7872e-02,  1.5283e-02],\n",
      "        ...,\n",
      "        [ 7.9460e-05, -2.5979e-02, -2.2147e-02,  ..., -1.2353e-02,\n",
      "          3.3651e-03,  1.5633e-02],\n",
      "        [-1.7197e-02,  3.8710e-02,  1.3586e-02,  ..., -4.5836e-02,\n",
      "          2.3277e-02,  1.5239e-02],\n",
      "        [-4.0602e-02, -1.1937e-02, -2.7950e-02,  ..., -2.9979e-03,\n",
      "         -2.9408e-03,  4.0283e-02]], device='cuda:0', requires_grad=True)\n",
      "name: fc1.bias, param: Parameter containing:\n",
      "tensor([ 3.7771e-02, -3.3374e-03,  4.3543e-02,  2.2222e-02, -3.5893e-02,\n",
      "        -1.1107e-02, -4.1721e-02,  1.0101e-02,  4.2407e-02, -9.1137e-03,\n",
      "         8.5668e-03,  4.7003e-02, -1.4925e-02, -4.1461e-02, -3.6369e-02,\n",
      "        -8.4031e-04, -4.7341e-02,  4.5063e-02, -4.2895e-02, -3.7032e-02,\n",
      "         6.9285e-03,  3.2819e-02, -4.6186e-03,  2.7063e-02,  4.7585e-02,\n",
      "        -5.1850e-03, -2.1936e-02, -1.2774e-05, -4.3951e-02,  1.8868e-02,\n",
      "        -2.9551e-02, -3.6412e-03, -2.3364e-02, -1.6555e-02,  4.8151e-02,\n",
      "         1.3602e-02,  2.4689e-02, -4.0113e-02,  2.2367e-02, -3.5706e-02,\n",
      "        -1.3717e-02, -2.0553e-02,  7.8678e-03, -2.6503e-02,  2.0263e-02,\n",
      "         2.0792e-03, -3.0403e-02,  4.9616e-02, -2.0091e-02, -1.4915e-02,\n",
      "        -1.7258e-02,  1.6341e-02,  7.2194e-03, -1.9264e-02,  3.5209e-02,\n",
      "        -3.9191e-02, -1.9932e-02, -2.4007e-02,  6.4631e-03,  3.4682e-04,\n",
      "        -4.7564e-02,  3.3563e-02,  1.3633e-02, -4.9941e-02, -2.5577e-02,\n",
      "        -2.2467e-02,  4.8476e-02,  2.1128e-02,  4.8906e-02,  1.7561e-02,\n",
      "         1.0153e-02,  2.0337e-02, -2.3452e-02,  4.9008e-02,  1.9780e-02,\n",
      "         8.7540e-03,  3.7378e-02, -1.8521e-03, -4.5266e-02,  4.9397e-02,\n",
      "         2.7623e-02,  2.0353e-02,  1.3558e-02,  2.2453e-02, -3.6492e-02,\n",
      "        -1.6875e-02,  3.0134e-02, -3.6188e-02, -3.1094e-02, -6.7064e-03,\n",
      "        -1.0915e-02, -2.5964e-02, -2.6054e-02,  4.1476e-02, -1.0526e-02,\n",
      "        -2.2365e-02, -8.8304e-03, -1.6516e-02,  2.1929e-02, -4.4478e-02,\n",
      "         4.3606e-02, -3.3391e-02, -4.2355e-02, -2.9443e-02,  1.5214e-02,\n",
      "        -4.8795e-02,  2.8856e-02, -4.2241e-02,  4.2184e-02,  3.9680e-02,\n",
      "        -3.3692e-03, -8.9229e-03,  3.4989e-02,  4.1897e-02, -4.7754e-02,\n",
      "         1.9010e-03, -3.7156e-02, -4.3778e-02, -1.4442e-02, -7.6985e-03],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "name: fc2.weight, param: Parameter containing:\n",
      "tensor([[ 0.0115,  0.0820,  0.0432,  ...,  0.0077,  0.0671, -0.0435],\n",
      "        [ 0.0630,  0.0777,  0.0907,  ...,  0.0750, -0.0809,  0.0129],\n",
      "        [ 0.0595, -0.0844,  0.0796,  ...,  0.0127, -0.0829, -0.0046],\n",
      "        ...,\n",
      "        [ 0.0160,  0.0898,  0.0018,  ..., -0.0800,  0.0423, -0.0583],\n",
      "        [-0.0300,  0.0703,  0.0830,  ...,  0.0826, -0.0781,  0.0853],\n",
      "        [-0.0412, -0.0571, -0.0183,  ...,  0.0277, -0.0648,  0.0684]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "name: fc2.bias, param: Parameter containing:\n",
      "tensor([ 0.0556, -0.0878,  0.0867, -0.0598,  0.0765, -0.0684, -0.0689,  0.0050,\n",
      "         0.0841,  0.0660,  0.0382,  0.0703,  0.0772,  0.0518, -0.0361,  0.0851,\n",
      "        -0.0678,  0.0081, -0.0797, -0.0258,  0.0311, -0.0882, -0.0643,  0.0185,\n",
      "         0.0324,  0.0003,  0.0468, -0.0116,  0.0004, -0.0231, -0.0730, -0.0223,\n",
      "         0.0710, -0.0840,  0.0516, -0.0197, -0.0072, -0.0743, -0.0822, -0.0733,\n",
      "        -0.0495, -0.0572,  0.0125,  0.0084, -0.0565,  0.0892, -0.0835, -0.0279,\n",
      "         0.0201, -0.0179,  0.0460,  0.0132,  0.0648, -0.0403,  0.0220, -0.0854,\n",
      "         0.0693, -0.0477, -0.0332, -0.0806,  0.0630, -0.0115,  0.0249, -0.0439,\n",
      "        -0.0025,  0.0131, -0.0777,  0.0223, -0.0022, -0.0486,  0.0498,  0.0235,\n",
      "         0.0596,  0.0478,  0.0130,  0.0524,  0.0197, -0.0483,  0.0306,  0.0494],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "name: fc3.weight, param: Parameter containing:\n",
      "tensor([[ 1.6030e-02, -5.4019e-02, -8.8484e-02, -9.0232e-02, -8.2844e-02,\n",
      "          2.0815e-03, -5.7518e-02,  7.8592e-02,  1.9294e-03,  1.2915e-02,\n",
      "         -8.1970e-02, -1.0535e-02,  1.1127e-01, -7.3773e-02, -4.5957e-02,\n",
      "         -5.7905e-02, -6.9882e-02,  3.8742e-02, -9.7458e-02,  8.7135e-02,\n",
      "         -2.5192e-02,  9.8456e-02,  7.6081e-02,  1.1449e-02,  7.4436e-02,\n",
      "         -8.6180e-02,  4.7884e-02,  5.8924e-02, -6.3221e-02, -1.0027e-01,\n",
      "          9.4132e-02,  5.2178e-02,  8.8697e-02, -4.8770e-02, -9.0095e-02,\n",
      "         -9.1901e-02,  6.1902e-02, -4.2976e-03, -1.9732e-02, -5.0272e-02,\n",
      "          9.9901e-02, -7.2558e-02,  6.0711e-02, -6.6083e-02,  5.1117e-02,\n",
      "          4.6613e-02,  1.9198e-02, -4.4600e-02, -8.7107e-02, -1.5591e-02,\n",
      "          2.7380e-02, -5.4388e-02, -8.7345e-02, -5.8162e-02, -3.2533e-03,\n",
      "          4.4255e-02,  5.8449e-02, -8.1054e-02,  5.5705e-02,  1.1335e-02,\n",
      "          9.3672e-02,  5.8158e-02,  8.1651e-02,  1.9793e-02, -1.0575e-01,\n",
      "         -5.9996e-02,  6.4935e-02, -1.4092e-03, -6.5189e-02,  2.9883e-02,\n",
      "          8.3835e-02, -3.7048e-02,  9.7451e-02, -9.9420e-02, -8.6617e-02,\n",
      "          1.1114e-01,  1.0053e-02, -5.1913e-02,  4.3832e-02, -1.0737e-01],\n",
      "        [ 1.0414e-01,  5.3733e-02, -5.2995e-02,  1.7002e-02,  6.5304e-02,\n",
      "          6.7663e-03,  1.5733e-02,  4.4703e-02, -3.6759e-03, -7.6178e-02,\n",
      "         -3.4643e-02, -2.9585e-02, -8.9675e-02, -5.1247e-02, -3.1784e-02,\n",
      "          8.5355e-02, -5.9052e-02, -9.4845e-02, -8.8603e-02,  8.1098e-02,\n",
      "         -9.1450e-02,  1.0686e-01,  5.7493e-02, -3.0639e-02,  8.6911e-02,\n",
      "          1.6335e-02, -9.1176e-02, -4.4584e-02, -3.8141e-02, -1.5244e-02,\n",
      "         -8.2098e-02,  1.2750e-02, -1.5444e-02,  1.2748e-02,  2.3436e-02,\n",
      "         -7.5927e-02, -5.7834e-02,  1.0472e-01,  3.7889e-03, -1.0899e-01,\n",
      "          8.1587e-02,  9.6297e-02, -9.9298e-02,  9.1020e-02, -9.3343e-02,\n",
      "         -9.7984e-02, -8.5281e-02,  3.2468e-02,  3.8647e-02, -4.8228e-02,\n",
      "         -6.3622e-03, -3.0394e-02,  8.7955e-02, -1.7125e-02, -2.3649e-02,\n",
      "          1.1141e-01,  4.0807e-02,  1.0198e-01, -6.3043e-02, -6.3709e-02,\n",
      "          7.3941e-02, -2.0470e-02, -3.3395e-02, -1.6517e-02, -4.9188e-02,\n",
      "          8.3999e-04, -3.4547e-02, -7.3287e-02, -7.6875e-02,  2.7627e-02,\n",
      "          4.5546e-02, -1.1026e-01,  7.4121e-02,  9.9309e-02, -7.0253e-02,\n",
      "          3.3153e-02, -9.4179e-03,  1.0633e-01,  1.0450e-01, -1.1076e-01],\n",
      "        [-9.1143e-02,  1.0441e-01, -9.1288e-02, -6.3410e-02, -5.6754e-02,\n",
      "          5.3106e-02,  3.0054e-02, -1.1936e-02, -1.0438e-01, -4.2892e-03,\n",
      "          3.2465e-02, -1.6807e-02, -3.5277e-03,  4.1999e-02, -1.1344e-02,\n",
      "          3.6886e-03,  8.1722e-02,  3.1485e-02,  7.0284e-02, -2.8634e-02,\n",
      "          1.2179e-02,  9.3933e-02, -2.0779e-03,  8.9825e-02, -8.6641e-02,\n",
      "          9.8287e-02, -7.7159e-02, -7.6076e-02, -4.6066e-02, -3.7485e-02,\n",
      "          9.6220e-02,  2.4997e-03,  9.8683e-02, -6.7383e-02,  8.0783e-02,\n",
      "          7.1193e-02, -1.0437e-01,  7.0027e-02,  1.5109e-02, -1.0265e-01,\n",
      "          7.9289e-02, -3.5063e-02,  3.1769e-02,  4.3404e-03,  3.1837e-03,\n",
      "          8.8876e-02, -7.1798e-02, -3.4192e-03, -6.3687e-02, -1.0136e-01,\n",
      "          4.2271e-02, -6.6742e-02, -7.1734e-02,  1.0159e-01,  1.6313e-02,\n",
      "         -4.6265e-02, -2.5594e-02,  7.8619e-02,  2.3908e-02, -9.7081e-02,\n",
      "          1.0489e-01, -3.9865e-02,  5.1986e-02, -5.3348e-02,  4.3929e-02,\n",
      "          5.2141e-03, -7.5269e-02, -3.0326e-02,  5.3865e-02,  5.6373e-02,\n",
      "          2.9230e-02,  4.5647e-02, -5.7677e-02,  3.5743e-02,  1.0071e-01,\n",
      "          3.4074e-02, -1.2343e-02, -4.9610e-02, -1.0887e-01,  5.6684e-02],\n",
      "        [-1.6276e-02,  7.7647e-02,  2.1162e-02,  3.3640e-02, -2.1501e-02,\n",
      "         -3.2021e-02,  8.1313e-02, -5.7678e-02,  2.7962e-02, -4.4930e-03,\n",
      "         -8.2300e-02, -6.3019e-02,  4.5127e-02, -7.4986e-02,  1.0739e-01,\n",
      "          1.0659e-01, -9.5256e-02,  9.5331e-02, -1.0814e-01, -3.7448e-02,\n",
      "         -5.0280e-03, -7.9479e-02, -7.2897e-02,  5.3232e-02, -7.5792e-02,\n",
      "          3.2885e-02,  5.7097e-02, -4.4414e-02, -1.0037e-01,  5.2863e-02,\n",
      "          2.0413e-02, -3.9785e-02,  1.0411e-01, -1.0710e-01,  5.7141e-03,\n",
      "          4.1202e-02, -9.4933e-02, -2.0337e-02, -2.9651e-02, -3.0195e-02,\n",
      "          6.8663e-02, -4.5631e-02,  8.4688e-02, -6.9861e-02, -1.0075e-01,\n",
      "          2.8775e-02, -6.1937e-02, -1.5899e-02,  5.5975e-02, -2.6865e-02,\n",
      "          7.6911e-02,  5.3086e-02,  4.4943e-02,  8.2728e-02, -9.6581e-02,\n",
      "          5.0165e-02,  9.4116e-02,  5.3673e-02, -2.0379e-02, -3.7368e-03,\n",
      "          5.2904e-02,  5.4713e-02,  1.0919e-01, -9.4792e-02, -8.4047e-02,\n",
      "         -1.3193e-02,  7.5768e-02,  2.5893e-02, -6.8682e-02, -7.4467e-02,\n",
      "          3.8096e-02, -1.0156e-01,  5.6432e-02,  5.2323e-02, -8.5137e-02,\n",
      "          2.3571e-02, -6.6449e-02,  1.9774e-02,  4.6815e-02, -9.6398e-02],\n",
      "        [-1.0362e-01,  1.0647e-02, -3.6360e-02, -2.0880e-02, -4.3625e-02,\n",
      "         -5.1833e-02, -1.0854e-01,  3.3613e-02,  3.2428e-02, -1.0143e-01,\n",
      "          5.9683e-02, -3.7076e-02,  1.7651e-02,  5.4509e-02, -8.5362e-02,\n",
      "          1.7504e-02,  1.0271e-01,  6.3136e-02,  1.6814e-02, -9.9013e-02,\n",
      "          9.4500e-02,  8.2830e-02, -2.1648e-02, -5.5674e-03, -3.0613e-02,\n",
      "         -1.0331e-01, -1.4793e-02, -1.4637e-03, -1.0208e-02, -2.1353e-02,\n",
      "          6.6722e-02, -3.7378e-02, -7.9310e-02, -6.8165e-02,  1.0932e-01,\n",
      "          3.6869e-02,  8.1461e-02,  6.8971e-02, -5.3919e-02,  3.4371e-02,\n",
      "          5.8847e-02, -1.7038e-02, -3.0052e-02, -6.0557e-02,  1.9003e-02,\n",
      "         -9.8926e-02,  3.4808e-03,  9.4323e-02, -6.6188e-02,  9.0432e-02,\n",
      "          9.5230e-03,  1.1002e-01, -5.1828e-02,  6.6279e-02,  6.3275e-02,\n",
      "         -7.5730e-02, -7.0948e-02,  4.5196e-02, -3.9166e-02,  4.8060e-02,\n",
      "          8.0392e-02, -9.6502e-02,  2.2165e-02,  2.4784e-02, -3.1023e-02,\n",
      "         -1.0122e-01, -2.7184e-02,  7.3321e-02, -3.8130e-02, -7.0616e-02,\n",
      "         -8.8133e-02, -7.1598e-02,  4.9248e-02, -3.7063e-02,  6.2389e-02,\n",
      "          6.6911e-02, -9.7372e-02,  3.4521e-02, -4.1607e-03,  1.0847e-01],\n",
      "        [ 2.4514e-03,  4.1135e-02, -8.3209e-02,  2.7096e-02,  3.6342e-02,\n",
      "         -9.6187e-02, -2.4394e-02,  1.1042e-01, -2.7171e-03,  1.3977e-02,\n",
      "          8.9460e-02, -2.8177e-02, -2.8182e-02,  1.0337e-01, -4.3021e-02,\n",
      "         -1.7045e-02, -2.8081e-03,  7.3856e-02, -6.0120e-03,  4.6189e-02,\n",
      "         -4.4741e-02, -1.0547e-01,  4.6803e-02, -8.0824e-02, -2.1515e-02,\n",
      "          3.4272e-02,  1.3269e-02,  8.8754e-02,  3.9811e-03, -2.4492e-02,\n",
      "          1.0894e-01, -7.6150e-02,  6.6131e-02, -8.8691e-02,  4.2648e-02,\n",
      "         -1.0133e-02,  3.8929e-02, -5.8883e-02,  4.1839e-02, -6.7733e-03,\n",
      "         -2.6489e-02, -3.6568e-02, -1.1123e-01,  4.4604e-02, -9.5497e-02,\n",
      "          6.3262e-02,  3.0184e-02, -8.5662e-02, -6.6326e-03, -8.3630e-02,\n",
      "          4.9114e-02, -5.6568e-02,  3.6089e-02,  4.2392e-02, -8.8210e-03,\n",
      "         -9.9354e-02, -1.0401e-01,  3.8528e-02,  1.0274e-01, -2.2057e-02,\n",
      "         -1.4663e-03, -2.2143e-03, -1.0602e-01, -1.2909e-02,  1.3717e-02,\n",
      "          2.5098e-02,  9.9300e-02,  7.7396e-02, -8.4271e-02,  4.0494e-02,\n",
      "         -9.5550e-02, -7.8470e-02, -1.0893e-01,  5.8200e-02,  1.0593e-01,\n",
      "         -3.3251e-03,  7.4467e-02,  2.7877e-02,  2.2200e-02,  4.4970e-02],\n",
      "        [ 1.0247e-02,  2.8464e-02, -2.4647e-02, -1.0050e-01,  1.1009e-01,\n",
      "          8.7695e-02,  4.6579e-02,  2.5757e-02, -1.9304e-03, -7.6348e-02,\n",
      "          9.0309e-02,  2.8529e-02, -8.8883e-02,  6.4415e-02, -1.0413e-01,\n",
      "          1.0212e-01, -8.8803e-02,  7.9875e-02,  7.1954e-02, -4.2259e-02,\n",
      "          1.0680e-01,  1.0792e-01, -1.5811e-02,  4.1178e-02,  1.2104e-02,\n",
      "          5.3702e-02, -4.2868e-02,  6.5307e-02, -8.0298e-02, -6.6997e-02,\n",
      "         -6.1282e-02,  7.5883e-02, -1.6539e-02, -6.8983e-02, -4.6888e-02,\n",
      "         -3.4842e-02, -8.8741e-03,  8.9714e-02,  7.2252e-02, -3.6565e-02,\n",
      "         -4.5263e-02,  9.2044e-02,  6.3287e-02, -8.5879e-02,  5.0023e-02,\n",
      "         -5.6794e-02,  7.8391e-02, -2.4546e-02, -8.6229e-02, -6.9918e-02,\n",
      "          8.3843e-02,  8.1957e-02, -2.7157e-02, -8.2479e-02, -7.6709e-02,\n",
      "         -9.8479e-02, -7.3213e-02, -1.0789e-01,  7.7986e-02,  5.0930e-02,\n",
      "          7.1741e-02,  1.3585e-02, -6.2861e-02, -1.5985e-02, -8.3370e-02,\n",
      "          1.0235e-01, -3.9063e-02, -9.6077e-02, -5.6780e-02, -1.0734e-02,\n",
      "         -8.3473e-02, -4.8821e-02, -3.9882e-02,  9.7011e-02,  1.0956e-01,\n",
      "         -6.1405e-02, -1.1699e-02, -4.7851e-02, -2.7648e-04, -8.1673e-02],\n",
      "        [ 7.4737e-02, -9.7275e-03,  5.9551e-02, -9.6448e-02, -9.2483e-02,\n",
      "          8.0983e-02, -2.7586e-02, -7.1171e-02,  2.3990e-02,  8.9259e-02,\n",
      "         -1.1056e-02,  1.0427e-01, -1.0585e-01,  7.4149e-02, -7.4352e-02,\n",
      "          2.8340e-02,  1.0409e-01,  9.7683e-02, -3.2917e-03,  6.0092e-02,\n",
      "         -6.3301e-02, -9.0750e-02, -4.1062e-02, -9.6965e-02,  4.6992e-04,\n",
      "          3.4579e-02,  7.4433e-02, -1.3040e-02, -6.7055e-02, -3.7469e-02,\n",
      "         -9.4391e-02, -1.5838e-02, -7.5875e-02, -8.8838e-02,  4.1348e-02,\n",
      "         -6.1846e-02,  7.2415e-02, -1.0173e-01, -3.8177e-02, -5.5724e-02,\n",
      "         -1.0531e-01,  5.2384e-02, -8.4313e-02, -1.0356e-01,  5.5119e-02,\n",
      "          3.6088e-02,  8.1855e-02,  3.9490e-02, -7.1208e-02, -8.1537e-02,\n",
      "         -6.9561e-02,  5.5563e-02,  1.2647e-02,  9.8924e-02, -4.9655e-02,\n",
      "         -2.1725e-02,  1.8295e-03,  3.8842e-02, -9.8271e-02, -8.1616e-02,\n",
      "         -9.3109e-02, -9.9704e-02,  2.7481e-02, -6.5966e-02, -9.2001e-02,\n",
      "          9.6537e-02, -8.4730e-02,  2.2309e-02, -7.4414e-02,  1.0930e-01,\n",
      "          1.7862e-02, -4.3993e-02,  9.5234e-02,  8.5622e-02,  7.8856e-02,\n",
      "         -4.3378e-02, -2.1581e-02,  7.4180e-02, -2.3169e-02, -5.1014e-02],\n",
      "        [-7.9610e-02, -5.9187e-02,  6.0338e-02, -5.1730e-02,  3.8314e-02,\n",
      "         -7.7186e-03, -5.2037e-02,  9.9915e-02,  9.5463e-02,  4.6325e-04,\n",
      "         -6.9971e-02,  8.3020e-02,  9.0392e-02, -6.0575e-02, -1.0313e-01,\n",
      "         -3.5134e-03, -1.0432e-01,  5.2080e-02, -6.2991e-02,  2.1884e-02,\n",
      "         -4.4884e-02, -2.4089e-02,  1.1039e-01, -5.2511e-02,  4.5428e-02,\n",
      "         -5.5413e-02, -7.3504e-02,  4.1793e-02,  3.4962e-02,  1.0325e-02,\n",
      "         -4.8434e-02,  1.4954e-02,  6.8081e-02, -1.1120e-01, -7.7220e-02,\n",
      "          7.1981e-02,  5.5328e-02,  1.5600e-03, -6.4188e-02,  6.2002e-02,\n",
      "          1.0416e-01, -5.2219e-02, -6.6587e-02,  3.1860e-02, -1.0987e-01,\n",
      "          8.6915e-02,  5.4853e-02, -4.9090e-02,  9.3083e-02,  3.5814e-02,\n",
      "          1.7445e-02, -1.0133e-01, -3.9455e-02,  3.0299e-02, -9.0762e-02,\n",
      "         -8.3220e-02,  8.6951e-02, -2.3130e-02, -2.4921e-02, -4.4542e-02,\n",
      "          5.5967e-02,  1.0532e-01, -7.2024e-02,  9.2252e-02,  5.1705e-02,\n",
      "         -9.8946e-02,  5.9550e-02, -3.4479e-02,  5.2443e-02,  3.1337e-02,\n",
      "          4.8916e-02,  7.2293e-03, -7.1157e-02, -7.9513e-02,  5.4983e-02,\n",
      "         -9.9104e-03,  5.8706e-02,  4.7395e-04,  4.0980e-02,  1.0502e-01],\n",
      "        [ 2.2216e-02,  9.0389e-02,  1.4210e-02, -5.0814e-02,  8.9122e-02,\n",
      "         -9.4164e-02, -6.6717e-02, -1.0780e-02, -1.1039e-01, -5.1843e-02,\n",
      "         -7.6460e-02, -8.5513e-02, -6.8101e-02,  1.0706e-01, -1.0062e-01,\n",
      "          5.3900e-02,  1.1070e-01,  5.7281e-02, -2.5068e-02,  1.2909e-02,\n",
      "          8.8485e-02,  5.2428e-02, -9.7863e-02, -2.7933e-02, -9.6675e-02,\n",
      "         -8.1014e-02,  3.4686e-02, -9.5845e-02, -2.7383e-02,  1.0969e-04,\n",
      "          8.5660e-02,  1.0867e-01, -7.8289e-02,  4.0770e-02,  2.9340e-02,\n",
      "          2.0365e-02, -8.8759e-02,  8.8675e-02,  5.0885e-02,  1.0110e-01,\n",
      "         -5.8524e-02, -2.0935e-02,  4.1629e-02, -3.3550e-02, -2.0782e-02,\n",
      "         -4.9886e-02,  8.9573e-02,  4.5166e-02, -3.5243e-03, -9.7594e-02,\n",
      "          2.5416e-02, -1.0687e-01,  7.6546e-02,  6.9537e-02, -8.0660e-02,\n",
      "          6.6111e-02, -8.3786e-02, -2.7118e-02,  4.5362e-03,  1.4762e-02,\n",
      "          2.8160e-02,  6.4511e-02, -6.0382e-02, -3.1670e-02,  7.5409e-02,\n",
      "          8.0202e-03, -8.8311e-02,  7.9342e-02, -1.1782e-02,  2.6526e-02,\n",
      "          5.8017e-02, -9.9329e-02, -9.3459e-02, -8.0947e-02,  6.6629e-02,\n",
      "         -2.1367e-02, -1.0861e-01,  2.2017e-02,  2.7770e-02,  7.8111e-02]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "name: fc3.bias, param: Parameter containing:\n",
      "tensor([ 0.0147, -0.0423,  0.0926, -0.0195,  0.0477,  0.0464, -0.0518,  0.0099,\n",
      "        -0.0114,  0.0354], device='cuda:0', requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for name, parameters in model.named_parameters():   # 各层参数及具体数字\n",
    "    print('name: {}, param: {}'.format(name, parameters))\n",
    "# for n, c in model.named_children():    # 各层名称与具体定义\n",
    "#     print(\"name:{}, children:{}\".format(n,c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "print(obj.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataParallel(\n",
      "  (module): LeNet(\n",
      "    (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "    (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "    (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
      "    (fc2): Linear(in_features=120, out_features=80, bias=True)\n",
      "    (fc3): Linear(in_features=80, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(obj.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: tensor([[[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          ...,\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]],\n",
      "\n",
      "\n",
      "        [[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          ...,\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]],\n",
      "\n",
      "\n",
      "        [[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          ...,\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          ...,\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]],\n",
      "\n",
      "\n",
      "        [[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          ...,\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]],\n",
      "\n",
      "\n",
      "        [[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          ...,\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]]]) torch.Size([64, 1, 32, 32])\n",
      "Y: tensor([4, 7, 7, 9, 8, 3, 2, 3, 0, 6, 2, 5, 5, 7, 6, 4, 8, 0, 0, 0, 4, 6, 6, 8,\n",
      "        3, 4, 5, 3, 7, 2, 1, 4, 2, 5, 8, 6, 0, 8, 1, 7, 4, 3, 2, 6, 8, 7, 2, 5,\n",
      "        1, 3, 9, 6, 2, 3, 8, 9, 2, 1, 6, 6, 7, 1, 9, 5]) torch.Size([64])\n",
      "tensor([[[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          ...,\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]],\n",
      "\n",
      "\n",
      "        [[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          ...,\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]],\n",
      "\n",
      "\n",
      "        [[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          ...,\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          ...,\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]],\n",
      "\n",
      "\n",
      "        [[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          ...,\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]],\n",
      "\n",
      "\n",
      "        [[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          ...,\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]]],\n",
      "       device='cuda:0') torch.Size([64, 1, 32, 32])\n",
      "tensor([4, 7, 7, 9, 8, 3, 2, 3, 0, 6, 2, 5, 5, 7, 6, 4, 8, 0, 0, 0, 4, 6, 6, 8,\n",
      "        3, 4, 5, 3, 7, 2, 1, 4, 2, 5, 8, 6, 0, 8, 1, 7, 4, 3, 2, 6, 8, 7, 2, 5,\n",
      "        1, 3, 9, 6, 2, 3, 8, 9, 2, 1, 6, 6, 7, 1, 9, 5], device='cuda:0') torch.Size([64])\n",
      "Fx: tensor([[-0.0528, -0.0320,  0.0355,  0.0306,  0.0227,  0.0723, -0.0201,  0.0366,\n",
      "          0.0928, -0.0065],\n",
      "        [-0.0535, -0.0606,  0.0325,  0.0244,  0.0262,  0.0677, -0.0312,  0.0404,\n",
      "          0.0752,  0.0014],\n",
      "        [-0.0558, -0.0659,  0.0468,  0.0132,  0.0217,  0.0797, -0.0211,  0.0270,\n",
      "          0.0919, -0.0061],\n",
      "        [-0.0429, -0.0435,  0.0429,  0.0289,  0.0414,  0.0654, -0.0416,  0.0262,\n",
      "          0.0720, -0.0012],\n",
      "        [-0.0372, -0.0410,  0.0478,  0.0395,  0.0112,  0.0608, -0.0273,  0.0354,\n",
      "          0.1008, -0.0267],\n",
      "        [-0.0402, -0.0216,  0.0391,  0.0530,  0.0338,  0.0552, -0.0376,  0.0423,\n",
      "          0.0675,  0.0028],\n",
      "        [-0.0264, -0.0187,  0.0636,  0.0358,  0.0236,  0.0475, -0.0456,  0.0322,\n",
      "          0.0719, -0.0033],\n",
      "        [-0.0158, -0.0257,  0.0621,  0.0493,  0.0210,  0.0850, -0.0182,  0.0385,\n",
      "          0.0910, -0.0136],\n",
      "        [-0.0300, -0.0660,  0.0410,  0.0189,  0.0325,  0.0805,  0.0035,  0.0356,\n",
      "          0.1100, -0.0077],\n",
      "        [-0.0447, -0.0555,  0.0274,  0.0196,  0.0454,  0.0804, -0.0248,  0.0282,\n",
      "          0.0865,  0.0049],\n",
      "        [-0.0404, -0.0548,  0.0434,  0.0235,  0.0338,  0.0610, -0.0138,  0.0376,\n",
      "          0.0851, -0.0105],\n",
      "        [-0.0315, -0.0614,  0.0363,  0.0196,  0.0407,  0.0956, -0.0094,  0.0440,\n",
      "          0.1043, -0.0178],\n",
      "        [-0.0184, -0.0433,  0.0435,  0.0193,  0.0271,  0.0546, -0.0306,  0.0310,\n",
      "          0.0840,  0.0036],\n",
      "        [-0.0256, -0.0361,  0.0594,  0.0302,  0.0271,  0.0649, -0.0320,  0.0357,\n",
      "          0.0823, -0.0011],\n",
      "        [-0.0394, -0.0704,  0.0446,  0.0096,  0.0641,  0.0913, -0.0096,  0.0178,\n",
      "          0.0918,  0.0026],\n",
      "        [-0.0397, -0.0559,  0.0251,  0.0198,  0.0539,  0.0875, -0.0242,  0.0362,\n",
      "          0.0942,  0.0186],\n",
      "        [-0.0187, -0.0264,  0.0580,  0.0320,  0.0374,  0.0718, -0.0273,  0.0276,\n",
      "          0.0938, -0.0172],\n",
      "        [-0.0347, -0.0484,  0.0409,  0.0209,  0.0247,  0.0762, -0.0190,  0.0361,\n",
      "          0.0758,  0.0054],\n",
      "        [-0.0338, -0.0750,  0.0341,  0.0144,  0.0358,  0.0732, -0.0069,  0.0351,\n",
      "          0.0987, -0.0030],\n",
      "        [-0.0445, -0.0680,  0.0487,  0.0165,  0.0352,  0.0728, -0.0205,  0.0454,\n",
      "          0.0778, -0.0021],\n",
      "        [-0.0541, -0.0696,  0.0350,  0.0086,  0.0320,  0.0858, -0.0346,  0.0236,\n",
      "          0.1031, -0.0109],\n",
      "        [-0.0156, -0.0624,  0.0448,  0.0232,  0.0318,  0.0640, -0.0094,  0.0387,\n",
      "          0.0799, -0.0090],\n",
      "        [-0.0366, -0.0508,  0.0128,  0.0243,  0.0535,  0.0631, -0.0290,  0.0245,\n",
      "          0.0781,  0.0010],\n",
      "        [-0.0362, -0.0341,  0.0495,  0.0336,  0.0146,  0.0773, -0.0356,  0.0247,\n",
      "          0.0989, -0.0089],\n",
      "        [-0.0460, -0.0395,  0.0299,  0.0326,  0.0397,  0.0865, -0.0162,  0.0321,\n",
      "          0.0861,  0.0080],\n",
      "        [-0.0480, -0.0460,  0.0438,  0.0180,  0.0174,  0.0865, -0.0088,  0.0215,\n",
      "          0.0923, -0.0022],\n",
      "        [-0.0524, -0.0581,  0.0247,  0.0473,  0.0177,  0.0700, -0.0291,  0.0466,\n",
      "          0.1044, -0.0024],\n",
      "        [-0.0572, -0.0643,  0.0217,  0.0578,  0.0359,  0.0664, -0.0361,  0.0331,\n",
      "          0.0870, -0.0075],\n",
      "        [-0.0486, -0.0374,  0.0247,  0.0418,  0.0180,  0.0641, -0.0356,  0.0315,\n",
      "          0.0775, -0.0175],\n",
      "        [-0.0328, -0.0489,  0.0158,  0.0259,  0.0432,  0.0958, -0.0061,  0.0384,\n",
      "          0.0834,  0.0128],\n",
      "        [-0.0236, -0.0096,  0.0464,  0.0484,  0.0225,  0.0552, -0.0475,  0.0306,\n",
      "          0.0682, -0.0040],\n",
      "        [-0.0426, -0.0699,  0.0081,  0.0319,  0.0579,  0.0879, -0.0382,  0.0308,\n",
      "          0.1009, -0.0083],\n",
      "        [-0.0235, -0.0186,  0.0537,  0.0535,  0.0304,  0.0468, -0.0358,  0.0318,\n",
      "          0.0928, -0.0225],\n",
      "        [-0.0374, -0.0781,  0.0319,  0.0472,  0.0270,  0.0674, -0.0247,  0.0376,\n",
      "          0.0952, -0.0156],\n",
      "        [-0.0406, -0.0538,  0.0391,  0.0314,  0.0189,  0.0731, -0.0352,  0.0424,\n",
      "          0.0987, -0.0258],\n",
      "        [-0.0419, -0.0623,  0.0309,  0.0074,  0.0518,  0.0784, -0.0173,  0.0129,\n",
      "          0.1022, -0.0095],\n",
      "        [-0.0370, -0.0753,  0.0316,  0.0389,  0.0167,  0.0972, -0.0174,  0.0108,\n",
      "          0.1385, -0.0093],\n",
      "        [-0.0565, -0.0454,  0.0369,  0.0393,  0.0046,  0.0746, -0.0426,  0.0355,\n",
      "          0.1134, -0.0117],\n",
      "        [-0.0099, -0.0244,  0.0309,  0.0429,  0.0310,  0.0521, -0.0511,  0.0247,\n",
      "          0.0759, -0.0220],\n",
      "        [-0.0721, -0.0578,  0.0134,  0.0280,  0.0346,  0.0784, -0.0401,  0.0439,\n",
      "          0.0871, -0.0007],\n",
      "        [-0.0408, -0.0760,  0.0411,  0.0185,  0.0448,  0.0908, -0.0118,  0.0219,\n",
      "          0.0788, -0.0013],\n",
      "        [-0.0396, -0.0180,  0.0399,  0.0468,  0.0289,  0.0728, -0.0234,  0.0233,\n",
      "          0.0919, -0.0072],\n",
      "        [-0.0364, -0.0213,  0.0447,  0.0465,  0.0213,  0.0557, -0.0480,  0.0301,\n",
      "          0.0866, -0.0095],\n",
      "        [-0.0424, -0.0608,  0.0446,  0.0170,  0.0331,  0.0920,  0.0050,  0.0385,\n",
      "          0.1067, -0.0144],\n",
      "        [-0.0106, -0.0346,  0.0670,  0.0444,  0.0275,  0.0673, -0.0327,  0.0216,\n",
      "          0.0929, -0.0187],\n",
      "        [-0.0519, -0.0503,  0.0189,  0.0333,  0.0289,  0.0725, -0.0338,  0.0468,\n",
      "          0.0717,  0.0061],\n",
      "        [-0.0319, -0.0473,  0.0369,  0.0338,  0.0390,  0.0645, -0.0374,  0.0318,\n",
      "          0.0781,  0.0051],\n",
      "        [-0.0315, -0.0467,  0.0422,  0.0318,  0.0408,  0.0968, -0.0218,  0.0243,\n",
      "          0.1089, -0.0086],\n",
      "        [-0.0121, -0.0054,  0.0586,  0.0437,  0.0267,  0.0599, -0.0383,  0.0308,\n",
      "          0.0657,  0.0009],\n",
      "        [-0.0409, -0.0456,  0.0324,  0.0522,  0.0399,  0.0734, -0.0259,  0.0313,\n",
      "          0.1020,  0.0115],\n",
      "        [-0.0570, -0.0566,  0.0392,  0.0233,  0.0404,  0.0829, -0.0425,  0.0283,\n",
      "          0.0926, -0.0082],\n",
      "        [-0.0503, -0.0606,  0.0424,  0.0115,  0.0321,  0.0838, -0.0131,  0.0500,\n",
      "          0.0890,  0.0010],\n",
      "        [-0.0468, -0.0504,  0.0409,  0.0177,  0.0335,  0.0808, -0.0343,  0.0418,\n",
      "          0.0853,  0.0068],\n",
      "        [-0.0368, -0.0124,  0.0209,  0.0516,  0.0286,  0.0587, -0.0585,  0.0378,\n",
      "          0.0726, -0.0002],\n",
      "        [-0.0156, -0.0437,  0.0440,  0.0292,  0.0309,  0.0817, -0.0093,  0.0308,\n",
      "          0.0862, -0.0125],\n",
      "        [-0.0399, -0.0316,  0.0516,  0.0339,  0.0237,  0.0677, -0.0339,  0.0446,\n",
      "          0.0671,  0.0016],\n",
      "        [-0.0317,  0.0002,  0.0500,  0.0487,  0.0197,  0.0600, -0.0243,  0.0407,\n",
      "          0.0853, -0.0126],\n",
      "        [-0.0165, -0.0454,  0.0357,  0.0360,  0.0494,  0.0613, -0.0291,  0.0098,\n",
      "          0.0774,  0.0045],\n",
      "        [-0.0379, -0.0648,  0.0453,  0.0190,  0.0531,  0.0918,  0.0114,  0.0458,\n",
      "          0.0939, -0.0041],\n",
      "        [-0.0258, -0.0361,  0.0473,  0.0153,  0.0265,  0.0796, -0.0178,  0.0271,\n",
      "          0.1005, -0.0182],\n",
      "        [-0.0672, -0.0347,  0.0238,  0.0333,  0.0118,  0.0831, -0.0308,  0.0386,\n",
      "          0.0907, -0.0108],\n",
      "        [-0.0114, -0.0286,  0.0281,  0.0366,  0.0376,  0.0524, -0.0569,  0.0285,\n",
      "          0.0689, -0.0087],\n",
      "        [-0.0402, -0.0539,  0.0340,  0.0305,  0.0390,  0.0801, -0.0457,  0.0299,\n",
      "          0.0911,  0.0033],\n",
      "        [-0.0307, -0.0535,  0.0545,  0.0377,  0.0324,  0.0804, -0.0214,  0.0365,\n",
      "          0.0890, -0.0176]], device='cuda:0', grad_fn=<GatherBackward>) torch.Size([64, 10])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loss: tensor(2.2915, device='cuda:0', grad_fn=<NllLossBackward>) <class 'torch.Tensor'>\n",
      "Pred: tensor([[8],\n",
      "        [8],\n",
      "        [8],\n",
      "        [8],\n",
      "        [8],\n",
      "        [8],\n",
      "        [8],\n",
      "        [8],\n",
      "        [8],\n",
      "        [8],\n",
      "        [8],\n",
      "        [8],\n",
      "        [8],\n",
      "        [8],\n",
      "        [8],\n",
      "        [8],\n",
      "        [8],\n",
      "        [5],\n",
      "        [8],\n",
      "        [8],\n",
      "        [8],\n",
      "        [8],\n",
      "        [8],\n",
      "        [8],\n",
      "        [5],\n",
      "        [8],\n",
      "        [8],\n",
      "        [8],\n",
      "        [8],\n",
      "        [5],\n",
      "        [8],\n",
      "        [8],\n",
      "        [8],\n",
      "        [8],\n",
      "        [8],\n",
      "        [8],\n",
      "        [8],\n",
      "        [8],\n",
      "        [8],\n",
      "        [8],\n",
      "        [5],\n",
      "        [8],\n",
      "        [8],\n",
      "        [8],\n",
      "        [8],\n",
      "        [5],\n",
      "        [8],\n",
      "        [8],\n",
      "        [8],\n",
      "        [8],\n",
      "        [8],\n",
      "        [8],\n",
      "        [8],\n",
      "        [8],\n",
      "        [8],\n",
      "        [5],\n",
      "        [8],\n",
      "        [8],\n",
      "        [8],\n",
      "        [8],\n",
      "        [8],\n",
      "        [8],\n",
      "        [8],\n",
      "        [8]], device='cuda:0') torch.Size([64, 1])\n",
      "Z: tensor([[4],\n",
      "        [7],\n",
      "        [7],\n",
      "        [9],\n",
      "        [8],\n",
      "        [3],\n",
      "        [2],\n",
      "        [3],\n",
      "        [0],\n",
      "        [6],\n",
      "        [2],\n",
      "        [5],\n",
      "        [5],\n",
      "        [7],\n",
      "        [6],\n",
      "        [4],\n",
      "        [8],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [4],\n",
      "        [6],\n",
      "        [6],\n",
      "        [8],\n",
      "        [3],\n",
      "        [4],\n",
      "        [5],\n",
      "        [3],\n",
      "        [7],\n",
      "        [2],\n",
      "        [1],\n",
      "        [4],\n",
      "        [2],\n",
      "        [5],\n",
      "        [8],\n",
      "        [6],\n",
      "        [0],\n",
      "        [8],\n",
      "        [1],\n",
      "        [7],\n",
      "        [4],\n",
      "        [3],\n",
      "        [2],\n",
      "        [6],\n",
      "        [8],\n",
      "        [7],\n",
      "        [2],\n",
      "        [5],\n",
      "        [1],\n",
      "        [3],\n",
      "        [9],\n",
      "        [6],\n",
      "        [2],\n",
      "        [3],\n",
      "        [8],\n",
      "        [9],\n",
      "        [2],\n",
      "        [1],\n",
      "        [6],\n",
      "        [6],\n",
      "        [7],\n",
      "        [1],\n",
      "        [9],\n",
      "        [5]], device='cuda:0') 64\n",
      "pred eq tensor([[0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [1],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [1],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [1],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [1],\n",
      "        [0],\n",
      "        [0],\n",
      "        [1],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [1],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [1],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0]], device='cuda:0', dtype=torch.uint8)\n",
      "Correct: tensor(7, device='cuda:0')\n",
      "Acc: tensor(0.1094, device='cuda:0')\n",
      "epoch Loss: 2.2915236949920654\n",
      "epoch Acc: 0.109375\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    877\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 878\u001b[0;31m                 \u001b[0mident\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdin_socket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    879\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/jupyter_client/session.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self, socket, mode, content, copy)\u001b[0m\n\u001b[1;32m    802\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 803\u001b[0;31m             \u001b[0mmsg_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_multipart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    804\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mzmq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZMQError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/zmq/sugar/socket.py\u001b[0m in \u001b[0;36mrecv_multipart\u001b[0;34m(self, flags, copy, track)\u001b[0m\n\u001b[1;32m    466\u001b[0m         \"\"\"\n\u001b[0;32m--> 467\u001b[0;31m         \u001b[0mparts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    468\u001b[0m         \u001b[0;31m# have first part already, only loop while more to receive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._recv_copy\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/zmq/backend/cython/checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-d6f9f981a7b3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"epoch Loss:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"epoch Acc:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_acc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m     \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"2:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    851\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    852\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 853\u001b[0;31m             \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    854\u001b[0m         )\n\u001b[1;32m    855\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    881\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 883\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    884\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    885\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# print(len(list(obj.train_iterator)))   # 844 iterator；每次iterator 64个样本（batch_size);每个样本（32*32）\n",
    "# for (x, y) in obj.train_iterator:\n",
    "#     print(x, len(x))\n",
    "#     print(\"***\"*30)\n",
    "#     print(x[0][0][0], len(x[0][0][0]))\n",
    "#     print(\"***\"*30)\n",
    "#     print(y, len(y))\n",
    "#     input()\n",
    "\n",
    "def accu(fx, y):\n",
    "    pred = fx.max(1, keepdim=True)[1]\n",
    "    print(\"Pred:\", pred, pred.shape)\n",
    "    z = y.view_as(pred)   # 把y的size变成和pred的一样，后面好比较\n",
    "    print(\"Z:\", z, len(z))\n",
    "    print(\"pred eq\", pred.eq(z))\n",
    "    correct = pred.eq(y.view_as(pred)).sum()  # 得到该batch的准确度\n",
    "    print(\"Correct:\", correct)\n",
    "    acc = correct.float()/pred.shape[0]\n",
    "    return acc\n",
    "\n",
    "epoch_loss = 0   # 积累变量\n",
    "epoch_acc = 0    # 积累变量\n",
    "obj.model.train() \n",
    "for (x, y) in obj.train_iterator:  \n",
    "    print(\"X:\",x, x.shape)       # x  torch.Size([64, 1, 32, 32])\n",
    "    print(\"Y:\",y, y.shape)       # y torch.Size([64]\n",
    "    x = x.to(obj.device)\n",
    "    y = y.to(obj.device)\n",
    "    print(x, x.shape)\n",
    "    print(y, y.shape)\n",
    "    \n",
    "    obj.optimizer.zero_grad()   # 清空梯度计算损失\n",
    "    fx = obj.model(x)           # 进行forward\n",
    "    print(\"Fx:\", fx, fx.shape)   # torch.Size([64, 10]  # 每个类的概率\n",
    "    input() \n",
    "    loss = obj.criterion(fx,y)  # 计算Loss,train_loss\n",
    "    print(\"Loss:\", loss, type(loss))\n",
    "    acc = accu(fx,y)     # 计算精确度，train_accu\n",
    "    print(\"Acc:\", acc)\n",
    "    \n",
    "    loss.backward()          # 进行BP\n",
    "    obj.optimizer.step()    # 统一更新模型 #所有的optimizer都实现了step()方法，这个方法会更新所有的参数,这是大多数optimizer所支持的简化版本。一旦梯度被如backward()之类的函数计算好后，我们就可以调用这个函数。\n",
    "    epoch_loss += loss.item()\n",
    "    epoch_acc += acc.item()\n",
    "    print(\"epoch Loss:\", epoch_loss)\n",
    "    print(\"epoch Acc:\", epoch_acc)\n",
    "    input(\"2:\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:1 | Train Loss:0.2314406042141725 | Train Acc:0.930002221563981 | Val Loss:0.09761913215860407 | Val Acc:0.9704122340425532\n",
      "Cost time: 0.642307702700297 mins.\n"
     ]
    }
   ],
   "source": [
    "obj.train_fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Test Loss: 0.07367058542029113 | Test Acc: 0.9765127388535032 |\n"
     ]
    }
   ],
   "source": [
    "obj.get_acc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss的内部是怎么计算的？"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
