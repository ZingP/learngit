# -*- coding: utf-8 -*-
"""CNN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1r1ue0EFaFH298BmjDqJltmm_1FFs9FDg
"""

from tensorflow.examples.tutorials.mnist import input_data

mnist = input_data.read_data_sets('./MNIST_data',one_hot=True)

import os 
import random
import numpy as np

model_dir='models'
if not os.path.isdir(model_dir):
    os.makedirs(model_dir)

# 不像TensorFlow一样有GPU和CPU版本，安装的时候pytorch会自动识别CPU还是GPU
import torch
# nn就是各种层
import torch.nn as nn
import torch.nn.functional as F
# optim是优化器
import torch.optim as optim
# torchvision有optim的一些东西
import torchvision
# trainsforms相当于pipeline
import torchvision.transforms as transforms
import torchvision.datasets as datasets

def accu(fx,y):
    pred = fx.max(1,keepdim=True)[1]
    correct = pred.eq(y.view_as(pred)).sum()#得到该batch的准确度
    acc = correct.float()/pred.shape[0]
    return acc

def train(model,device,iterator,optimizer,criterion):
    epoch_loss = 0#积累变量
    epoch_acc = 0#积累变量
    model.train()#该函数表示PHASE=Train
    
    for (x,y) in iterator:#拿去每一个minibatch
        x = x.to(device)
        y = y.to(device)
        optimizer.zero_grad()
        fx = model(x)#进行forward
        loss = criterion(fx,y)#计算Loss,train_loss
        type(loss)
        acc=accu(fx,y)#计算精确度，train_accu
        loss.backward()#进行BP
        optimizer.step()#统一更新模型
        epoch_loss += loss.item()
        epoch_acc += acc.item()
        
    return epoch_loss/len(iterator),epoch_acc/len(iterator)

def evaluate(model,device,iterator,criterion):
    epoch_loss = 0
    epoch_acc = 0
    model.eval()
    with torch.no_grad():
        for (x,y) in iterator:
            x = x.to(device)
            y = y.to(device)
            fx = model(x)
            loss = criterion(fx,y)
            acc = accu(fx,y)
            epoch_loss += loss.item()
            epoch_acc += acc.item()
    return epoch_loss/len(iterator),epoch_acc/len(iterator)

data_trans_alexnet = transforms.Compose([
    transforms.Resize(227),
    transforms.ToTensor(),
    transforms.Normalize((0.1307,),(0.3081,))#参数mean和std来自于训练集，但是transform本身会在训练和评测的时候都会使用
])
train_data = datasets.MNIST('data', train=True, download=True,
                            transform=data_trans_alexnet)
test_data = datasets.MNIST('data', train=False, download=True,
                           transform=data_trans_alexnet)

n_train = int(len(train_data)*0.9)
n_validation = len(train_data)-n_train

train_data,valid_data = torch.utils.data.random_split(train_data,
                                                      [n_train,n_validation])

print(len(train_data),len(valid_data),len(test_data))

batch_size = 64

train_iterator = torch.utils.data.DataLoader(train_data,shuffle=True,
                                             batch_size=batch_size)
valid_iterator = torch.utils.data.DataLoader(valid_data,batch_size=batch_size)
test_iterator = torch.utils.data.DataLoader(test_data,batch_size=batch_size)

class AlexNet(nn.Module):
    def __init__(self):#init函数定义的是网络的架构、关键的网络模块、模组
        super(AlexNet,self).__init__()
        self.feature_block=nn.Sequential(
            nn.Conv2d(1,64,kernel_size=11,stride=4,padding=2),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=3,stride=2),
            nn.Conv2d(64,192,kernel_size=5,padding=2),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=3,stride=2),
            nn.Conv2d(192,384,kernel_size=3,padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(384,256,kernel_size=3,padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(256,256,kernel_size=3,padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=3,stride=2)
        )
        self.avgpool=nn.AdaptiveAvgPool2d((6,6))
        self.class_block=nn.Sequential(
            nn.Dropout(),
            nn.Linear(256*6*6,4096),
            nn.ReLU(inplace=True),
            nn.Dropout(),
            nn.Linear(4096,4096),
            nn.ReLU(inplace=True),
            nn.Linear(4096,10),
        )
    def forward(self,x):#数据的正向流
        x = self.feature_block(x)
        x = self.avgpool(x)
        x = x.view(x.size(0),256*6*6)
        x = self.class_block(x)
        return x

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu:0')

device

model = AlexNet().to(device)

model

optimizer = optim.Adam(model.parameters())

criterion = nn.CrossEntropyLoss()

epochs = 10
model_dir = 'models'
model_path = os.path.join(model_dir,'alexnet_mnist.pt')

best_valid_loss = float('inf')

import time

info = 'Epoch:{0}|Train Loss:{1}|Train Acc:{2}|Val Loss:{3}|Val Acc:{4}'
start = time.time()
for epoch in range(epochs):
    train_loss,train_acc = train(model,device,train_iterator,optimizer,criterion)
    valid_loss,valid_acc = evaluate(model,device,valid_iterator,criterion)
    if valid_loss < best_valid_loss:#如果是最好的模型就保存到文件夹
        best_valid_loss = valid_loss
        torch.save(model.state_dict(),model_path)
    print(info.format(epoch+1,train_loss,train_acc,valid_loss,valid_acc))
end = time.time()
print("Cost time {} s".format(end-start))

model.load_state_dict(torch.load(model_path))
test_loss, test_acc = evaluate(model, device, test_iterator, criterion)
print('| Test Loss: {0} | Test Acc: {1} |'.format(test_loss,test_acc))

