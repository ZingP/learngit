{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## mnist手写数字识别"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-2-c651a1ed2de4>:2: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /root/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From /root/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting ./MNIST_data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From /root/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting ./MNIST_data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /root/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting ./MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ./MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /root/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "# 如果MNIST_data不存在会先下载再读入，pytorch的数据集和这个不一样。\n",
    "mnist = input_data.read_data_sets('./MNIST_data',one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(55000, 784)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 28*28*1 灰度图被拉成一行了\n",
    "mnist.train.images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 784)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.test.images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 784)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 验证集\n",
    "mnist.validation.images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 循环读，共55000张\n",
    "train_x,train_y = mnist.train.next_batch(20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000, 784)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试集\n",
    "test_x, test_y = mnist.test.next_batch(5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 图像的可视化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_x' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-a24dac1884b3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m# 打印图用imshow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_y\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxticks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_x' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.rc('image',cmap='binary')\n",
    "for i in range(10):#打印10张图\n",
    "    plt.subplot(2,5,i+1)\n",
    "    # 打印图用imshow\n",
    "    plt.imshow(train_x[i].reshape(28,28))\n",
    "    print(train_y[i])\n",
    "    plt.xticks(())\n",
    "    plt.yticks(())\n",
    "plt.tight_layout()\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 全连接神经网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Sequential 相当于pipeline，会自动执行forward和bp\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model1():\n",
    "    model=Sequential()\n",
    "    model.add(Dense(784,activation='relu'))\n",
    "    model.add(Dense(100,activation='relu'))\n",
    "    model.add(Dense(100,activation='relu'))\n",
    "    model.add(Dense(10,activation='softmax'))\n",
    "    model.compile(optimizer='adam',loss='categorical_crossentropy')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = model1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# m.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "20000/20000 [==============================] - 9s 472us/step - loss: 0.3123\n",
      "Epoch 2/10\n",
      "20000/20000 [==============================] - 6s 297us/step - loss: 0.1267\n",
      "Epoch 3/10\n",
      "20000/20000 [==============================] - 6s 297us/step - loss: 0.0812\n",
      "Epoch 4/10\n",
      "20000/20000 [==============================] - 6s 297us/step - loss: 0.0663\n",
      "Epoch 5/10\n",
      "20000/20000 [==============================] - 6s 298us/step - loss: 0.0455\n",
      "Epoch 6/10\n",
      "20000/20000 [==============================] - 6s 297us/step - loss: 0.0389\n",
      "Epoch 7/10\n",
      "20000/20000 [==============================] - 6s 300us/step - loss: 0.0325\n",
      "Epoch 8/10\n",
      "20000/20000 [==============================] - 6s 294us/step - loss: 0.0249\n",
      "Epoch 9/10\n",
      "20000/20000 [==============================] - 6s 297us/step - loss: 0.0224\n",
      "Epoch 10/10\n",
      "20000/20000 [==============================] - 6s 296us/step - loss: 0.0231\n"
     ]
    }
   ],
   "source": [
    "history = m.fit(train_x,train_y,epochs=10,batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = m.predict(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9678"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(pred.argmax(1),test_y.argmax(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 卷积神经网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import time\n",
    "import random\n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir='models'\n",
    "if not os.path.isdir(model_dir):\n",
    "    os.makedirs(model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LeNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 不像TensorFlow一样有GPU和CPU版本，安装的时候pytorch会自动识别CPU还是GPU\n",
    "import torch\n",
    "# nn就是各种层\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "# optim是优化器\n",
    "import torch.optim as optim\n",
    "# torchvision有optim的一些东西\n",
    "import torchvision\n",
    "# trainsforms相当于pipeline\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 定一个transforms\n",
    "# # compose相当于一个pipeline，链式调用\n",
    "# data_trans = transforms.Compose([\n",
    "#     transforms.Resize(32),   # resize是防止在多层网络中图像提前变成1*1\n",
    "#     transforms.ToTensor()\n",
    "# #    transforms.Normalize(()())?<-参数mean和std来自于训练集，但是transform本身会在训练和评测的时候都会使用\n",
    "# ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 图像的Normalize\n",
    "\n",
    "- 目的：将图片进行归一化的缩放|(x-mean)/std\n",
    "    - 如果不归一化，可能导致梯度爆炸或者梯度衰减、梯度为零。矩阵是相乘的，网络越深，连乘越多。如果梯度都大于1就会越乘越大，导致梯度爆炸，上溢出；如果梯度小于1，会越来越小，最终导致梯度为0，梯度离散，相当于平滩底部，学不到什么东西。\n",
    "\n",
    "- 思考：图片归一化后，真的不存在小于0或者大于1的outlier了吗？\n",
    "    - 不一定，如果数据不均匀，可能导致小于0或者大于1的数据。归一化的公式？\n",
    "\n",
    "- 思考：归一化哪部分数据？\n",
    "    - A 训练集 B 评测集 C 训练集+评测集？     A\n",
    "    - 不可能得到全部数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mnist' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-12a13909bf17>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmnist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'mnist' is not defined"
     ]
    }
   ],
   "source": [
    "np.mean(mnist.train.images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.30815944"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std(mnist.train.images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_trans = transforms.Compose([\n",
    "    transforms.Resize(32),   # 防止提前出现1*1像素的图像\n",
    "    transforms.ToTensor(),   # 数据结构转换成torch能接收的\n",
    "    transforms.Normalize((0.1307,),(0.3081,))  # 参数mean和std来自于训练集，但是transform本身会在训练和评测的时候都会使用\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 会建立一个data目录，train是否是训练集。\n",
    "train_data = datasets.MNIST('data', train=True, download=True, transform=data_trans)\n",
    "test_data = datasets.MNIST('data', train=False, download=True, transform=data_trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train = int(len(train_data)*0.9)\n",
    "n_validation = len(train_data)-n_train    # 10%做评测集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data,valid_data = torch.utils.data.random_split(train_data,[n_train,n_validation])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54000 6000 10000\n"
     ]
    }
   ],
   "source": [
    "print(len(train_data),len(valid_data),len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 目前完成了数据集的制作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterator 相当于pytorch里面的一个object，对数据进行切分之后的数据结构。只有训练集有必要shuffle。\n",
    "train_iterator=torch.utils.data.DataLoader(train_data,shuffle=True,batch_size=batch_size)\n",
    "valid_iterator=torch.utils.data.DataLoader(valid_data,batch_size=batch_size)\n",
    "test_iterator=torch.utils.data.DataLoader(test_data,batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet,self).__init__()\n",
    "        #第一层conv1卷积层，in_channel=1,output_channel=6,kernel_size=5*5,input_size=32*32,output_size=28*28\n",
    "        self.conv1=nn.Conv2d(1,6,5)\n",
    "        #第二层conv2，output_channel=6 ,kernel 5*5,output_size=10*10,input_size=14*14\n",
    "        self.conv2=nn.Conv2d(6,16,5)\n",
    "        self.fc1=nn.Linear(16*5*5,120)\n",
    "        self.fc2=nn.Linear(120,80)\n",
    "        self.fc3=nn.Linear(80,10)#不用增加softmax层，在cross_entropy的Loss中自动增加了Softmax\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x=F.max_pool2d(F.relu(self.conv1(x)),2)\n",
    "        x=F.max_pool2d(F.relu(self.conv2(x)),2)\n",
    "        x=x.view(x.shape[0],-1)  #？\n",
    "        x=F.relu(self.fc1(x))\n",
    "        x=F.relu(self.fc2(x))\n",
    "        x=self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 到此神经网络定义完毕\n",
    "\n",
    "## 载入模型并训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=LeNet().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LeNet(\n",
       "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
       "  (fc2): Linear(in_features=120, out_features=80, bias=True)\n",
       "  (fc3): Linear(in_features=80, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-01e6a7db4b03>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m# 优化器\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "optimizer=optim.Adam(model.parameters())   # 优化器\n",
    "criterion=nn.CrossEntropyLoss()    # loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion=nn.CrossEntropyLoss()    # loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 如何评价\n",
    "- 计算精确度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accu(fx,y):\n",
    "    pred=fx.max(1,keepdim=True)[1]\n",
    "    correct=pred.eq(y.view_as(pred)).sum()#得到该batch的准确度\n",
    "    acc=correct.float()/pred.shape[0]\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model,device,iterator,optimizer,criterion):\n",
    "    epoch_loss=0#积累变量\n",
    "    epoch_acc=0#积累变量\n",
    "    model.train()#该函数表示PHASE=Train\n",
    "    \n",
    "    for (x,y) in iterator:#拿去每一个minibatch\n",
    "        x=x.to(device)\n",
    "        y=y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        fx=model(x)#进行forward\n",
    "        loss=criterion(fx,y)#计算Loss,train_loss\n",
    "        type(loss)\n",
    "        acc=accu(fx,y)#计算精确度，train_accu\n",
    "        loss.backward()#进行BP\n",
    "        optimizer.step()#统一更新模型\n",
    "        epoch_loss+=loss.item()\n",
    "        epoch_acc+=acc.item()\n",
    "        \n",
    "    return epoch_loss/len(iterator),epoch_acc/len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model,device,iterator,criterion):\n",
    "    epoch_loss=0\n",
    "    epoch_acc=0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for (x,y) in iterator:\n",
    "            x=x.to(device)\n",
    "            y=y.to(device)\n",
    "            fx=model(x)\n",
    "            loss=criterion(fx,y)\n",
    "            acc=accu(fx,y)\n",
    "            epoch_loss+=loss.item()\n",
    "            epoch_acc+=acc.item()\n",
    "    return epoch_loss/len(iterator),epoch_acc/len(iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 开始训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=10\n",
    "model_path=os.path.join(model_dir,'lenet_mnist.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_valid_loss=float('inf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:1|Train Loss:0.23972646112584672|Train Acc:0.9244729957026893|Val Loss:0.09596572071313858|Val Acc:0.9720744680851063\n",
      "Epoch:2|Train Loss:0.0694747051506599|Train Acc:0.9781546208530806|Val Loss:0.06675227184561973|Val Acc:0.9782247340425532\n",
      "Epoch:3|Train Loss:0.05002978890560462|Train Acc:0.98362213669795|Val Loss:0.06574748136831368|Val Acc:0.9805518617021277\n",
      "Epoch:4|Train Loss:0.03945154916733433|Train Acc:0.9870347057897333|Val Loss:0.06790013114693871|Val Acc:0.981216755319149\n",
      "Epoch:5|Train Loss:0.03192989953012334|Train Acc:0.9900585011848341|Val Loss:0.06779796327711975|Val Acc:0.9798869680851063\n",
      "Epoch:6|Train Loss:0.025910181896010704|Train Acc:0.9915395438388626|Val Loss:0.05836796587546098|Val Acc:0.9850398936170213\n",
      "Epoch:7|Train Loss:0.023935663322940188|Train Acc:0.991928317535545|Val Loss:0.05189152884958411|Val Acc:0.9850398936170213\n",
      "Epoch:8|Train Loss:0.01990795230009633|Train Acc:0.9935759774881516|Val Loss:0.05808203031978956|Val Acc:0.984375\n",
      "Epoch:9|Train Loss:0.017008175816325143|Train Acc:0.994372037914692|Val Loss:0.058914484416550776|Val Acc:0.9833776595744681\n",
      "Epoch:10|Train Loss:0.015909737430654148|Train Acc:0.9947608116113744|Val Loss:0.059398194312729795|Val Acc:0.984375\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    train_loss,train_acc=train(model,device,train_iterator,optimizer,criterion)\n",
    "    valid_loss,valid_acc=evaluate(model,device,valid_iterator,criterion)\n",
    "    if valid_loss<best_valid_loss:#如果是最好的模型就保存到文件夹\n",
    "        best_valid_loss=valid_loss\n",
    "        torch.save(model.state_dict(),model_path)\n",
    "    print('Epoch:{0}|Train Loss:{1}|Train Acc:{2}|Val Loss:{3}|Val Acc:{4}'.format(epoch+1,train_loss,train_acc,valid_loss,valid_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Test Loss: 0.034779636307981365 | Test Acc: 0.9883558917197452 |\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "test_loss, test_acc = evaluate(model, device, test_iterator, criterion)\n",
    "\n",
    "print('| Test Loss: {0} | Test Acc: {1} |'.format(test_loss,test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AlexNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.set_num_threads(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_trans_alexnet = transforms.Compose([\n",
    "    transforms.Resize(227),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,),(0.3081,))#参数mean和std来自于训练集，但是transform本身会在训练和评测的时候都会使用\n",
    "])\n",
    "train_data = datasets.MNIST('data',train=True,download=True,transform=data_trans_alexnet)\n",
    "test_data = datasets.MNIST('data',train=False,download=True,transform=data_trans_alexnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train=int(len(train_data)*0.9)\n",
    "n_validation=len(train_data)-n_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data,valid_data=torch.utils.data.random_split(train_data,[n_train,n_validation])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54000 6000 10000\n"
     ]
    }
   ],
   "source": [
    "print(len(train_data),len(valid_data),len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iterator=torch.utils.data.DataLoader(train_data,shuffle=True,batch_size=batch_size)\n",
    "valid_iterator=torch.utils.data.DataLoader(valid_data,batch_size=batch_size)\n",
    "test_iterator=torch.utils.data.DataLoader(test_data,batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlexNet(nn.Module):\n",
    "    def __init__(self):#init函数定义的是网络的架构、关键的网络模块、模组\n",
    "        super(AlexNet,self).__init__()\n",
    "        self.feature_block=nn.Sequential(\n",
    "            nn.Conv2d(1,64,kernel_size=11,stride=4,padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3,stride=2),\n",
    "            nn.Conv2d(64,192,kernel_size=5,padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3,stride=2),\n",
    "            nn.Conv2d(192,384,kernel_size=3,padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(384,256,kernel_size=3,padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256,256,kernel_size=3,padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3,stride=2)\n",
    "        )\n",
    "        self.avgpool=nn.AdaptiveAvgPool2d((6,6))\n",
    "        self.class_block=nn.Sequential(\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(256*6*6,4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096,4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(4096,10),\n",
    "        )\n",
    "    def forward(self,x):#数据的正向流\n",
    "        x = self.feature_block(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0),256*6*6)\n",
    "        x = self.class_block(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AlexNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.device_count() > 1:\n",
    "    model = nn.DataParallel(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataParallel(\n",
       "  (module): AlexNet(\n",
       "    (feature_block): Sequential(\n",
       "      (0): Conv2d(1, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
       "      (1): ReLU(inplace)\n",
       "      (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "      (4): ReLU(inplace)\n",
       "      (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (7): ReLU(inplace)\n",
       "      (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (9): ReLU(inplace)\n",
       "      (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (11): ReLU(inplace)\n",
       "      (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n",
       "    (class_block): Sequential(\n",
       "      (0): Dropout(p=0.5)\n",
       "      (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
       "      (2): ReLU(inplace)\n",
       "      (3): Dropout(p=0.5)\n",
       "      (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "      (5): ReLU(inplace)\n",
       "      (6): Linear(in_features=4096, out_features=10, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer=optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion=nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=10\n",
    "model_dir='models'\n",
    "model_path=os.path.join(model_dir,'alexnet_mnist.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_valid_loss=float('inf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:1|Train Loss:0.32351489656415033|Train Acc:0.8941424763033176|Val Loss:0.07242300928114577|Val Acc:0.979332890282286\n",
      "Epoch:2|Train Loss:0.09779548081370751|Train Acc:0.972477290726386|Val Loss:0.10621865283935628|Val Acc:0.9686945924099456\n",
      "Epoch:3|Train Loss:0.08353047891215408|Train Acc:0.9770561809082167|Val Loss:0.05300462175596585|Val Acc:0.984375\n",
      "Epoch:4|Train Loss:0.07058588084566156|Train Acc:0.9801848835148518|Val Loss:0.04931169888500045|Val Acc:0.9858710106382979\n",
      "Epoch:5|Train Loss:0.06481341558389402|Train Acc:0.9818572274881516|Val Loss:0.04368885833661052|Val Acc:0.988530585106383\n",
      "Epoch:6|Train Loss:0.06335728050908074|Train Acc:0.9819683056872038|Val Loss:0.0384771905720551|Val Acc:0.9896387413461157\n",
      "Epoch:7|Train Loss:0.05232932414060626|Train Acc:0.9853932168246445|Val Loss:0.039285554669480374|Val Acc:0.9905806741815932\n",
      "Epoch:8|Train Loss:0.0590564951982561|Train Acc:0.9838319510503968|Val Loss:0.035191089238109506|Val Acc:0.9903590425531915\n",
      "Epoch:9|Train Loss:0.04876718690654183|Train Acc:0.9865718799603493|Val Loss:0.03700866214831934|Val Acc:0.9920212765957447\n",
      "Epoch:10|Train Loss:0.048491575686212506|Train Acc:0.9866706161137441|Val Loss:0.03971361452119147|Val Acc:0.989804964750371\n",
      "Cost time: 2103.1075785160065 s.\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "for epoch in range(epochs):\n",
    "    train_loss,train_acc=train(model,device,train_iterator,optimizer,criterion)\n",
    "    valid_loss,valid_acc=evaluate(model,device,valid_iterator,criterion)\n",
    "    if valid_loss<best_valid_loss:  # 如果是最好的模型就保存到文件夹\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(),model_path)\n",
    "    print('Epoch:{0}|Train Loss:{1}|Train Acc:{2}|Val Loss:{3}|Val Acc:{4}'.format(epoch+1,train_loss,train_acc,valid_loss,valid_acc))\n",
    "end = time.time()\n",
    "print(\"Cost time: {} s.\".format(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Test Loss: 0.03559933172147365 | Test Acc: 0.9906449044585988 |\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(model_path))\n",
    "test_loss, test_acc = evaluate(model, device, test_iterator, criterion)\n",
    "print('| Test Loss: {0} | Test Acc: {1} |'.format(test_loss,test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VGG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGGBlock(nn.Module):\n",
    "    def __init__(self,in_channels,out_channels,batch_norm):#在后来改良后的VGG网络增加了BatchNorm\n",
    "        super(VGGBlock,self).__init__()\n",
    "        stack=[]\n",
    "        stack.append(nn.Conv2d(in_channels,out_channels,kernel_size=3,padding=1))\n",
    "        if batch_norm:\n",
    "            stack.append(nn.BatchNorm2d(out_channels))\n",
    "        stack.append(nn.ReLU(inplace=True))\n",
    "        self.model_block=nn.Sequential(*stack)\n",
    "    def forward(self,x):\n",
    "        return self.model_block(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGGNet11(nn.Module):\n",
    "    def __init__(self,block,pool,batch_norm):#block是一个网络模组抽象，pool也是pooling层的抽象\n",
    "        super(VGGNet11,self).__init__()\n",
    "        self.feature_block=nn.Sequential(\n",
    "            block(1,64,batch_norm), #32*32\n",
    "            pool(kernel_size=2,stride=2),#16*16\n",
    "            block(64,128,batch_norm),\n",
    "            pool(kernel_size=2,stride=2),#8*8\n",
    "            block(128,256,batch_norm),\n",
    "            block(256,256,batch_norm),\n",
    "            pool(kernel_size=2,stride=2),#4*4\n",
    "            block(256,512,batch_norm),\n",
    "            block(512,512,batch_norm),\n",
    "            pool(kernel_size=2,stride=2),#2*2\n",
    "            block(512,512,batch_norm),\n",
    "            block(512,512,batch_norm),\n",
    "            pool(kernel_size=2,stride=2),#1*1\n",
    "        )\n",
    "        self.classifier=nn.Linear(512,10)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x=self.feature_block(x)\n",
    "        x=x.view(x.shape[0],-1)\n",
    "        x=self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGGNet16(nn.Module):\n",
    "    def __init__(self,block,pool,batch_norm): #block是一个网络模组抽象，pool也是pooling层的抽象\n",
    "        super(VGGNet16,self).__init__()\n",
    "        self.feature_block=nn.Sequential(\n",
    "            block(1,64,batch_norm), #32*32\n",
    "            block(64,64,batch_norm), #32*32\n",
    "            pool(kernel_size=2,stride=2),#16*16\n",
    "            block(64,128,batch_norm),\n",
    "            block(128,128,batch_norm),\n",
    "            pool(kernel_size=2,stride=2),#8*8\n",
    "            block(128,256,batch_norm),\n",
    "            block(256,256,batch_norm),\n",
    "            pool(kernel_size=2,stride=2),#4*4\n",
    "            block(256,512,batch_norm),\n",
    "            block(512,512,batch_norm),\n",
    "            block(512,512,batch_norm),\n",
    "            pool(kernel_size=2,stride=2),#2*2\n",
    "            block(512,512,batch_norm),\n",
    "            block(512,512,batch_norm),\n",
    "            block(512,512,batch_norm),\n",
    "            pool(kernel_size=2,stride=2),#1*1\n",
    "        )\n",
    "        self.classifier=nn.Linear(512,10)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x=self.feature_block(x)\n",
    "        x=x.view(x.shape[0],-1)\n",
    "        x=self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timer(func):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start = time.time()\n",
    "        func(*args,**kwargs)\n",
    "        end = time.time()\n",
    "        cost = end - start\n",
    "        print(\"Cost time: {} mins.\".format(cost/60)) \n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "@timer\n",
    "def train_fit():\n",
    "    best_valid_loss = float('inf')\n",
    "    info = 'Epoch:{0} | Train Loss:{1} | Train Acc:{2} | Val Loss:{3} | Val Acc:{4}'\n",
    "    for epoch in range(epochs):\n",
    "        train_loss, train_acc = train(model, device, train_iterator, optimizer, criterion)\n",
    "        valid_loss, valid_acc = evaluate(model, device, valid_iterator, criterion)\n",
    "        if valid_loss < best_valid_loss:  # 如果是最好的模型就保存到文件夹\n",
    "            best_valid_loss = valid_loss\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "        print(info.format(epoch+1, train_loss, train_acc, valid_loss, valid_acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VGGNet16(VGGBlock, nn.MaxPool2d, True)\n",
    "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu:0')\n",
    "if torch.cuda.device_count() > 1:\n",
    "    model = nn.DataParallel(model)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataParallel(\n",
       "  (module): VGGNet16(\n",
       "    (feature_block): Sequential(\n",
       "      (0): VGGBlock(\n",
       "        (model_block): Sequential(\n",
       "          (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace)\n",
       "        )\n",
       "      )\n",
       "      (1): VGGBlock(\n",
       "        (model_block): Sequential(\n",
       "          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace)\n",
       "        )\n",
       "      )\n",
       "      (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (3): VGGBlock(\n",
       "        (model_block): Sequential(\n",
       "          (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace)\n",
       "        )\n",
       "      )\n",
       "      (4): VGGBlock(\n",
       "        (model_block): Sequential(\n",
       "          (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace)\n",
       "        )\n",
       "      )\n",
       "      (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (6): VGGBlock(\n",
       "        (model_block): Sequential(\n",
       "          (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace)\n",
       "        )\n",
       "      )\n",
       "      (7): VGGBlock(\n",
       "        (model_block): Sequential(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace)\n",
       "        )\n",
       "      )\n",
       "      (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (9): VGGBlock(\n",
       "        (model_block): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace)\n",
       "        )\n",
       "      )\n",
       "      (10): VGGBlock(\n",
       "        (model_block): Sequential(\n",
       "          (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace)\n",
       "        )\n",
       "      )\n",
       "      (11): VGGBlock(\n",
       "        (model_block): Sequential(\n",
       "          (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace)\n",
       "        )\n",
       "      )\n",
       "      (12): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (13): VGGBlock(\n",
       "        (model_block): Sequential(\n",
       "          (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace)\n",
       "        )\n",
       "      )\n",
       "      (14): VGGBlock(\n",
       "        (model_block): Sequential(\n",
       "          (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace)\n",
       "        )\n",
       "      )\n",
       "      (15): VGGBlock(\n",
       "        (model_block): Sequential(\n",
       "          (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace)\n",
       "        )\n",
       "      )\n",
       "      (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (classifier): Linear(in_features=512, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters())   # 优化器\n",
    "criterion = nn.CrossEntropyLoss()    # loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "model_path = os.path.join(model_dir,'vggnet16_mnist.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:1 | Train Loss:0.19393341220535726 | Train Acc:0.9390612658448694 | Val Loss:0.07097932977720778 | Val Acc:0.9781139188624443\n",
      "Epoch:2 | Train Loss:0.06113600740165084 | Train Acc:0.982912470379147 | Val Loss:0.04401426494834905 | Val Acc:0.988530585106383\n",
      "Epoch:3 | Train Loss:0.04657110462177068 | Train Acc:0.9873432563661964 | Val Loss:0.040046898965188794 | Val Acc:0.9875886529049975\n",
      "Epoch:4 | Train Loss:0.03594857168061685 | Train Acc:0.9899967910977902 | Val Loss:0.03436292640547803 | Val Acc:0.9915226063829787\n",
      "Epoch:5 | Train Loss:0.0333620997722883 | Train Acc:0.9907064573459715 | Val Loss:0.033263351282778575 | Val Acc:0.9906914893617021\n",
      "Epoch:6 | Train Loss:0.0272576184391587 | Train Acc:0.9925145636096385 | Val Loss:0.04149616933724982 | Val Acc:0.9891954787234043\n",
      "Epoch:7 | Train Loss:0.024179436739531934 | Train Acc:0.9930576125592417 | Val Loss:0.04127165740554003 | Val Acc:0.9893062945376051\n",
      "Epoch:8 | Train Loss:0.02496145206384396 | Train Acc:0.9933353080568721 | Val Loss:0.04908663307891247 | Val Acc:0.9901374115588817\n",
      "Epoch:9 | Train Loss:0.019500640461494995 | Train Acc:0.9945201421800948 | Val Loss:0.04320783102686735 | Val Acc:0.9893062945376051\n",
      "Epoch:10 | Train Loss:0.01773022176734816 | Train Acc:0.9951680983412322 | Val Loss:0.03947758831163036 | Val Acc:0.9901374115588817\n",
      "Cost time: 48.601798272132875 mins.\n"
     ]
    }
   ],
   "source": [
    "train_fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Test Loss: 0.029480772674273535 | Test Acc: 0.9918391719745223 |\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(model_path))\n",
    "test_loss, test_acc = evaluate(model, device, test_iterator, criterion)\n",
    "print('| Test Loss: {0} | Test Acc: {1} |'.format(test_loss,test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.get_num_threads()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.current_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
