{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import jieba\n",
    "import keras\n",
    "from keras import utils\n",
    "from keras.preprocessing import sequence\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "from keras.layers import Embedding,Dense,GlobalAveragePooling1D,Input,Activation\n",
    "from keras.models import Model,Sequential\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.8  # 比例\n",
    "config.gpu_options.allow_growth = True    # 按需\n",
    "set_session(tf.Session(config=config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label_sentence(filename):\n",
    "    \"\"\"\n",
    "    input file content:\n",
    "    体育 sentence_1\n",
    "    ...\n",
    "    娱乐 sentence_n\n",
    "    \n",
    "    output dict:\n",
    "    {\n",
    "    \"体育\": [sentence_1, sentence_2, ..., sentence_n],\n",
    "    \"娱乐\":[...],\n",
    "    ...\n",
    "    }\n",
    "    \"\"\"\n",
    "    label_sentence_dic = {}    # 定义一个标签：样本的字典\n",
    "    with open(filename, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            label, sentence = line.strip().split('\\t')\n",
    "            if label not in label_sentence_dic:\n",
    "                label_sentence_dic[label] = [sentence]\n",
    "            else:\n",
    "                label_sentence_dic[label].append([sentence])\n",
    "    return label_sentence_dic\n",
    "        \n",
    "# 根据label_sentence_dic 划分训练集和测试集，可以不用在这里分\n",
    "def save2file(label_sentences_dict, train_ratio, training_file, testing_file):\n",
    "    data_out_training = open(training_file,'w',encoding='utf8')\n",
    "    data_out_testing = open(testing_file,'w',encoding='utf8')\n",
    "    for label in label_sentences_dict:\n",
    "        sentences = label_sentences_dict[label]\n",
    "        np.random.shuffle(sentences)\n",
    "        for i in range(0,len(sentences)):\n",
    "            if i < int(train_ratio*len(sentences)):\n",
    "                data_out_training.write(label+':'+sentences[i]+'\\n')\n",
    "            else:\n",
    "                data_out_testing.write(label+':'+sentences[i]+'\\n')\n",
    "    data_out_training.close()\n",
    "    data_out_testing.close()\n",
    "\n",
    "def filter_word_frec_dic(word_frec_dic, min_num):\n",
    "    word_frec_dic_new = {}\n",
    "    for word, num in word_frec_dic.items():\n",
    "        if num >= min_num:\n",
    "            word_frec_dic_new[word] = num\n",
    "    return word_frec_dic_new\n",
    "    \n",
    "# 获得词频字典\n",
    "def get_word_frec_dic(filename):\n",
    "    label_set = set()\n",
    "    # 统计文档词频\n",
    "    word_freq_dic = {}\n",
    "    special_symbol = ['', '\\t', ',', ':', ';', '。']\n",
    "    with open(filename, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            label, sentence = line.strip().split('\\t')\n",
    "            if len(label) != 0:\n",
    "                label_set.add(label) \n",
    "            word_list = jieba.cut(sentence)   # 生成器\n",
    "            for word in word_list:\n",
    "                word = word.strip()\n",
    "                if word in special_symbol:  # 过滤特殊符号\n",
    "                    continue\n",
    "                if word not in word_freq_dic:\n",
    "                    word_freq_dic[word] = 1\n",
    "                else:\n",
    "                    word_freq_dic[word] += 1\n",
    "    return word_freq_dic, label_set\n",
    "\n",
    "# 不需要考虑顺序，为Word、label建立索引\n",
    "def get_word_label_2index(word_frec_dic, label_set):\n",
    "    \"\"\"\n",
    "    input:\n",
    "        word_frac_dic: {\"word1\":123, \"word2\":345, ...}\n",
    "        label_set: {\"体育\", \"娱乐\", ...}\n",
    "    \n",
    "    output:\n",
    "        word2index: {\"word1\":0, \"word2\":1, ...}\n",
    "        label2index: {\"体育\":0, \"娱乐\":1, ...}\n",
    "    \"\"\"\n",
    "    word2index = {}\n",
    "    lenth = len(word_frec_dic)\n",
    "    for i, word in zip(range(lenth), word_frec_dic):\n",
    "        word2index[word] = i\n",
    "\n",
    "    label2index = {}\n",
    "    for i, label in zip(range(len(label_set)), label_set):\n",
    "        label2index[label] = i\n",
    "    return word2index, label2index\n",
    "\n",
    "\n",
    "def create_trainingdata(filename, word2index, label2index, padding_len=300):\n",
    "    with open(filename, 'r', encoding='utf-8') as f:\n",
    "        training_data = []\n",
    "        for line in f:\n",
    "            label, sentence = line.strip().split('\\t')\n",
    "            words = jieba.cut(sentence)\n",
    "            sentence_word_index = [word2index[word] for word in words if word in word2index]\n",
    "            label = [label2index[label]]\n",
    "            training_data.append((sentence_word_index, label)) # 将特征和标签放一起读取 然后随机打乱\n",
    "\n",
    "    np.random.shuffle(training_data)\n",
    "\n",
    "    X_train = []\n",
    "    y_train = []\n",
    "    for feas, label in training_data:\n",
    "        X_train.append(feas)\n",
    "        y_train.append(label)\n",
    "\n",
    "    # 相当于将词汇统一为300，多出的删除，少的补上0\n",
    "    X_train = sequence.pad_sequences(X_train, maxlen=padding_len, padding='post', truncating='post')\n",
    "    y_train = utils.to_categorical(y_train, num_classes=10)   # onehot\n",
    "    \n",
    "    X_train = np.array(X_train)  # 注意这里不是onehot\n",
    "    y_train = np.array(y_train)\n",
    "    return X_train, y_train\n",
    "\n",
    "# model\n",
    "def fasttext(vocab_size, embedding_dim, label_num):\n",
    "    model = Sequential() \n",
    "    model.add(Embedding(vocab_size, embedding_dim, input_length=300))\n",
    "    model.add(GlobalAveragePooling1D())\n",
    "    # model.add(Dense(label_num*5))  # 如果加上这一层，效果不好\n",
    "    model.add(Dense(label_num))\n",
    "    model.add(Activation(activation='sigmoid'))\n",
    "    model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "\n",
    "def save2file(word2index, filename):\n",
    "    data_out = open(filename, 'w', encoding='utf8')\n",
    "    for word, index in word2index.items():\n",
    "        data_out.write(word + ':'+str(index) + '\\n')\n",
    "    data_out.close()\n",
    "\n",
    "def train(filename, min_num=10, embedding_dim=300,batch_size=256, epochs=30):\n",
    "    \"\"\"\n",
    "    min_num = 10    # 单词最少出现的次数\n",
    "    \"\"\"\n",
    "    word_freq_dic, label_set = get_word_frec_dic(filename)\n",
    "    word_freq_filter_dic = filter_word_frec_dic(word_freq_dic, min_num)\n",
    "    word2index, label2index = get_word_label_2index(word_freq_filter_dic, label_set)\n",
    "    \n",
    "    # 此处需要把word2index和label2index存成文本\n",
    "    # save2file(word2index,'./data/word2index.txt')\n",
    "    # save2file(label2index,'./data/label2index.txt')\n",
    "\n",
    "    # 读取训练数据集 返回可以直接入模型的特征和标签\n",
    "    X_train, y_train = create_trainingdata(filename, word2index, label2index, padding_len=300)\n",
    "    print('X_train[0] is: ', X_train[0])\n",
    "    print('y_train.shape is: ', y_train.shape)\n",
    "\n",
    "    vocab_size = len(word2index)    \n",
    "    model = fasttext(vocab_size, embedding_dim, len(label2index))\n",
    "    # # 通过fit的callbacks参数将回调函数传入模型中，这个参数接收一个回调函数列表，你可以传入任意个回调函数\n",
    "    # callback_lists = [\n",
    "    #     keras.callbacks.EarlyStopping(monitor = 'acc',  # 监控模型的验证精度\n",
    "    #                                   patience = 1,),   # 如果精度在多于一轮的时间（即两轮）内不再改善，就中断训练\n",
    "    #     # ModelCheckpoint用于在每轮过后保存当前权重\n",
    "    #     keras.callbacks.ModelCheckpoint(filepath = 'news_classfication_textcnn.h5', # 目标文件的保存路径\n",
    "    #                                     # 这两个参数的意思是，如果val_loss没有改善，那么不需要覆盖模型文件，\n",
    "    #                                     # 这就可以始终保存在训练过程中见到的最佳模型\n",
    "    #                                     monitor = 'val_loss', save_best_only = True,)\n",
    "    # ]\n",
    "    # history = model.fit(training_feas,training_labels,batch_size=256,epochs=30,validation_split=0.3,callbacks=callback_lists)\n",
    "    history = model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.3)\n",
    "    model.save('fasttext_news_cf.model')\n",
    "    plot(history)\n",
    "    \n",
    "\n",
    "def plot(history):\n",
    "    plt.subplot(211)\n",
    "    plt.title(\"accuracy\")\n",
    "    plt.plot(history.history[\"acc\"], color=\"r\", label=\"train\")\n",
    "    plt.plot(history.history[\"val_acc\"], color=\"b\", label=\"validation\")\n",
    "    plt.legend(loc=\"best\")\n",
    "\n",
    "    plt.subplot(212)\n",
    "    plt.title(\"loss\")\n",
    "    plt.plot(history.history[\"loss\"], color=\"r\", label=\"train\")\n",
    "    plt.plot(history.history[\"val_loss\"], color=\"b\", label=\"validation\")\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = './data/cnews_train.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.924 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "train(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_sentence_dic, label_set = get_label_sentence(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(label_sentence_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "体育 500\n",
      "娱乐 500\n",
      "家居 500\n",
      "房产 500\n",
      "教育 500\n",
      "时尚 500\n",
      "时政 500\n",
      "游戏 500\n",
      "科技 500\n",
      "财经 500\n"
     ]
    }
   ],
   "source": [
    "for k in label_sentence_dic:\n",
    "    print(k, len(label_sentence_dic[k]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['我', '爱', '北京', '天安门']\n",
      "我\n",
      "爱\n",
      "北京\n",
      "天安门\n"
     ]
    }
   ],
   "source": [
    "s = \"我爱北京天安门\"\n",
    "print(jieba.lcut(s))\n",
    "for i in jieba.cut(s):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 3, 3, 3, 2, 8, 8, 8, 8],\n",
       "       [4, 5, 5, 0, 0, 0, 0, 0, 0, 0]], dtype=int32)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence.pad_sequences([[1,2,3,3,3,2,8,8,8,8,8,8,77,8,8],[4,5,5]], maxlen=10, padding='post', truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utils.to_categorical(range(10), num_classes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
