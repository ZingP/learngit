{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sklearn 中的数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import minmax_scale\n",
    "from sklearn.preprocessing import RobustScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## StandardScaler\n",
    "- 这个实质上就是将数据减去均值除以标准差，那为什么不直接做，而用StandardScaler呢，其好处是可以保存均值和方差供后续使用\n",
    "- 那么用StandardScaler的目的是什么呢，官网教程说的非常清楚了：数据集的标准化是许多机器学习估计的共同要求，如果一个特征对应的数据的方差很大，那么它可能支配目标函数，使得模型无法向预期那样学习其它特征。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RobustScaler\n",
    "- 如果数据有离群点，对数据进行均差和方差的标准化效果不好，这种情况可以使用robust_scale和RobustScaler，它们有对数据中心化和数据的缩放鲁棒性更强的参数；\n",
    "- 根据第1个四分位数和第3个四分位数之间的范围来缩放数据[这里我也不知道为什么用1-3的范围来缩放数据后会有什么明显的变化，下面的图示也不是非常清楚]\n",
    "- 接收参数中X为一个二维数组[n_samples,n_features]\n",
    "- 注意：在官网的例子中，是将离群点给删除了，这个函数并不能删除离群点"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LabelEncoder\n",
    "- 就是将特征列的类别变为0-num_class-1之间，即对不连续的数字和文本进行编号"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "?preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
